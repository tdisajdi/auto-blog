import os
import json
import datetime
import time
import requests
import feedparser
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import google.generativeai as genai # 1ë²ˆ ì½”ë“œì˜ ì•ˆì •ì ì¸ SDKë¡œ ë³µê·€
import re
import html
from bs4 import BeautifulSoup

# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (1ë²ˆ ì½”ë“œ ê¸°ì¤€ í†µì¼) ---
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
UNSPLASH_ACCESS_KEY = os.environ.get("UNSPLASH_ACCESS_KEY")
GMAIL_USER = os.environ.get("GMAIL_USER")
GMAIL_APP_PASSWORD = os.environ.get("GMAIL_APP_PASSWORD")

# Gemini ì„¤ì • (1ë²ˆ ì½”ë“œ ë°©ì‹)
genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-3-flash-preview')

# --- 0. íˆìŠ¤í† ë¦¬ ê´€ë¦¬ ---
def load_history(filepath):
    if not os.path.exists(filepath): return []
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except: return []

def save_history(filepath, history, new_items):
    cutoff = datetime.datetime.now() - datetime.timedelta(days=30)
    cleaned = []
    
    for item in history:
        try:
            d = datetime.datetime.strptime(item.get('date', '2000-01-01'), "%Y-%m-%d")
            if d >= cutoff: cleaned.append(item)
        except: continue
        
    today = datetime.datetime.now().strftime("%Y-%m-%d")
    for item in new_items:
        cleaned.append({"id": item['id'], "title": item['title'], "date": today})
        
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(cleaned, f, ensure_ascii=False, indent=4)

# --- 1. ë°ì´í„° ìˆ˜ì§‘ (ì›¹ ìŠ¤í¬ë˜í•‘ ì¶”ê°€) ---
def scrape_article_text(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        res = requests.get(url, headers=headers, timeout=5)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'html.parser')
        paragraphs = soup.find_all('p')
        text = " ".join([p.get_text() for p in paragraphs])
        return text[:3000] if len(text) > 100 else None 
    except Exception as e:
        print(f"Scraping failed for {url}: {e}")
        return None

def fetch_rss(url, category):
    items = []
    try:
        feed = feedparser.parse(url)
        cutoff = datetime.datetime.now() - datetime.timedelta(days=7)
        for entry in feed.entries:
            if 'published_parsed' in entry and entry.published_parsed:
                pub_date = datetime.datetime.fromtimestamp(time.mktime(entry.published_parsed))
                if pub_date < cutoff: continue
            
            print(f"Scraping: {entry.title}")
            raw_text = scrape_article_text(entry.link)
            if not raw_text:
                raw_text = (entry.summary if 'summary' in entry else entry.title)[:2000]
            
            items.append({
                "id": entry.link,
                "title": entry.title,
                "type": category,
                "raw": raw_text
            })
    except Exception as e:
        print(f"RSS Error ({url}): {e}")
    return items

def get_candidates(mode):
    items = []
    if mode == "TECH":
        urls = ["https://www.theverge.com/rss/index.xml", "https://techcrunch.com/feed/"]
    elif mode == "BIO":
        urls = ["https://news.google.com/rss/search?q=Biotech+OR+%22FDA+approval%22+OR+%22Clinical+Trial%22&hl=en-US&gl=US&ceid=US:en"]
    elif mode == "PATENT":
        urls = ["https://news.google.com/rss/search?q=Patent+OR+%22Technology+Innovation%22+OR+%22Future+Tech%22&hl=en-US&gl=US&ceid=US:en"]
    
    for u in urls: items.extend(fetch_rss(u, mode))
    return items

# --- 2. ì£¼ì œ ì„ ì • ---
def select_top_2(candidates, history, category_name):
    history_ids = [h['id'] for h in history]
    filtered = [c for c in candidates if c['id'] not in history_ids]
    
    if len(filtered) < 2: return filtered[:2]
    
    cand_txt = "\n".join([f"{i}. {c['title']}" for i, c in enumerate(filtered[:15])])
    
    prompt = f"""
    ì—­í• : ì „ë¬¸ íˆ¬ì/ê¸°ìˆ  ë¸”ë¡œê·¸ í¸ì§‘ì¥ 'ìŠ¤í¬(spo)'.
    ëª©í‘œ: {category_name} ë¶„ì•¼ì—ì„œ ì‹¬ì¸µ ë¶„ì„(Deep-Dive)ì´ ê°€ëŠ¥í•˜ê³  íˆ¬ììë“¤ì˜ ê´€ì‹¬ì´ ì§‘ì¤‘ë  ë‰´ìŠ¤ 2ê°œ ì„ ì •.
    
    [í›„ë³´êµ°]
    {cand_txt}
    
    ì¡°ê±´:
    1. ê¸°ìˆ ì  ì›ë¦¬ë‚˜ ì‹œì¥ íŒŒê¸‰ë ¥ì„ ë¶„ì„í•  ê±°ë¦¬ê°€ ìˆëŠ” ì£¼ì œ ìš°ì„ .
    2. ì˜¤ì§ ìˆ«ì 2ê°œë§Œ ë°˜í™˜ (ì˜ˆ: 1, 4).
    """
    try:
        res = model.generate_content(prompt)
        nums = [int(s) for s in re.findall(r'\b\d+\b', res.text)]
        if len(nums) >= 2:
            return [filtered[nums[0]], filtered[nums[1]]]
    except: pass
    return filtered[:2]

# --- 3. ê¸€ ì‘ì„± ---
def write_blog_post(topic1, topic2, category_name):
    print(f"Writing {category_name} Post with Gemini...")
    
    structure_instruction = """
    ê° ì£¼ì œë³„ë¡œ ë°˜ë“œì‹œ ì•„ë˜ 5ê°€ì§€ H2 íƒœê·¸ ì„¹ì…˜ì„ í¬í•¨í•´ì•¼ í•¨:
    1. <h2>1. ë°°ê²½ ë° ê°œìš” (The Context)</h2> : í˜„ ìƒí™©ì„ 3ì¤„ ìš”ì•½ ë¦¬ìŠ¤íŠ¸(<ul>)ë¡œ ì œì‹œ.
    2. <h2>2. ê¸°ìˆ ì  ë©”ì»¤ë‹ˆì¦˜ (Technical Deep-Dive)</h2> : <table>ì„ 1ê°œ ì´ìƒ ë°˜ë“œì‹œ í¬í•¨.
    3. <h2>3. ì‹œì¥ íŒë„ ë° ê²½ìŸì‚¬ ë¶„ì„ (Market Dynamics)</h2> : ê°ê´€ì ì¸ [ìˆ˜ì¹˜/ë°ì´í„°] í¬í•¨.
    4. <h2>4. ë¦¬ìŠ¤í¬ ë° í•œê³„ì  (Risk Factors)</h2> : ê·œì œ, ê²½ìŸ, ê¸°ìˆ ì  ì¥ë²½ ë¶„ì„.
    5. <h2>5. ìŠ¤í¬(spo)ì˜ ì¸ì‚¬ì´íŠ¸ (Actionable Insights)</h2> : ì‹œì‚¬ì .
    """
    glossary_rule = "ì–´ë ¤ìš´ 'ì „ë¬¸ ìš©ì–´'ëŠ” ë°˜ë“œì‹œ <u> íƒœê·¸ë¡œ ê°ì‹¸ì£¼ì„¸ìš”."
    bold_rule = "ë¬¸ë‹¨ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ 'í•µì‹¬ ë¬¸ì¥'ì€ ë°˜ë“œì‹œ <b> íƒœê·¸ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”."

    outline = model.generate_content(f"ì£¼ì œ1: {topic1['title']}\nì£¼ì œ2: {topic2['title']}\nìœ„ ë‘ ì£¼ì œë¡œ '{category_name} ì‹¬ì¸µ ë¶„ì„' ê°œìš” ì‘ì„±.").text
    
    p1_prompt = f"""
    ì—­í• : ì „ë¬¸ í…Œí¬/ë°”ì´ì˜¤ ë¶„ì„ê°€ 'ìŠ¤í¬(spo)'.
    ê°œìš”: {outline}
    ì£¼ì œ 1: {topic1['title']} / ì›ë¬¸ ë‚´ìš©: {topic1['raw']}
    {glossary_rule}\n{bold_rule}
    [ì‘ì„± ì§€ì¹¨] HTML íƒœê·¸ë§Œ ì¶œë ¥.
    <h1>[{category_name} ì‹¬ì¸µë¶„ì„] {topic1['title']}</h1>
    [IMAGE_PLACEHOLDER_1]
    {structure_instruction}
    [IMAGE_PLACEHOLDER_2]
    ì£¼ì œ 1ì˜ ë‚´ìš©ë§Œ ì‘ì„±.
    """
    part1 = re.sub(r"```[a-zA-Z]*\n?|```", "", model.generate_content(p1_prompt).text).strip()
    
    p2_prompt = f"""
    ì•ë¶€ë¶„: {part1}
    ì£¼ì œ 2: {topic2['title']} / ì›ë¬¸ ë‚´ìš©: {topic2['raw']}
    {glossary_rule}\n{bold_rule}
    [ì‘ì„± ì§€ì¹¨] ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ ì‘ì„±. HTML íƒœê·¸ë§Œ ì¶œë ¥.
    <br><hr style="border: 0; height: 1px; background: #ddd; margin: 40px 0;"><br>
    <h1>[{category_name} ì‹¬ì¸µë¶„ì„] {topic2['title']}</h1>
    [IMAGE_PLACEHOLDER_3]
    {structure_instruction}
    [IMAGE_PLACEHOLDER_4]
    <br><hr style="border: 0; height: 2px; background: #2c3e50; margin: 50px 0;"><br>
    <h2>ğŸ¯ í†µí•© ì¸ì‚¬ì´íŠ¸: ë‘ ë‰´ìŠ¤ê°€ ê·¸ë¦¬ëŠ” ë¯¸ë˜ (The Bridge)</h2>
    <h2>ğŸ“– ì˜¤ëŠ˜ì˜ ìš©ì–´ ì •ë¦¬ (Glossary)</h2>
    <h2>ğŸ” SEO ë° íƒœê·¸ ì •ë³´ (ì—…ë¡œë“œìš©)</h2>
    <hr style="border: 0; height: 1px; background: #eee; margin: 40px 0;">
    <p style="color:grey; font-size: 0.9em; text-align: center;">* ë³¸ ì½˜í…ì¸ ëŠ” ì •ë³´ ì œê³µì„ ëª©ì ìœ¼ë¡œ í•˜ë©°, íˆ¬ìì˜ ì±…ì„ì€ ë³¸ì¸ì—ê²Œ ìˆìŠµë‹ˆë‹¤. <br> Editor: ìŠ¤í¬(spo)</p>
    """
    part2 = re.sub(r"```[a-zA-Z]*\n?|```", "", model.generate_content(p2_prompt).text).strip()
    
    return part1 + "\n" + part2

# --- 4. ì´ë¯¸ì§€ ë° ì´ë©”ì¼ ì „ì†¡ ---
def get_image_tag(keyword, alt_text=""):
    search_query = f"{keyword} high quality"
    url = f"https://api.unsplash.com/search/photos?query={search_query}&per_page=1&orientation=landscape&client_id={UNSPLASH_ACCESS_KEY}"
    try:
        data = requests.get(url, timeout=5).json()
        img_url = data['results'][0]['urls']['regular']
        return f"""
        <figure style="margin: 30px 0;">
            <img src='{img_url}' alt='{alt_text}' style='width:100%; border-radius:12px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);'>
            <figcaption style='color:#666; font-size:13px; text-align:center; margin-top:10px;'>Source: Unsplash ({keyword})</figcaption>
        </figure>
        """
    except: return ""

def inject_images(html_text, t1, t2):
    try:
        k1_main = model.generate_content(f"Extract one main object noun from: {t1['title']}").text.strip()
        k1_sub = model.generate_content(f"Extract abstract concept from: {t1['title']}").text.strip()
        k2_main = model.generate_content(f"Extract one main object noun from: {t2['title']}").text.strip()
        k2_sub = model.generate_content(f"Extract abstract concept from: {t2['title']}").text.strip()
    except: 
        k1_main, k1_sub = "technology", "analysis"
        k2_main, k2_sub = "news", "future"
    
    html_text = html_text.replace("[IMAGE_PLACEHOLDER_1]", get_image_tag(k1_main, t1['title']))
    html_text = html_text.replace("[IMAGE_PLACEHOLDER_2]", get_image_tag(k1_sub + " visualization", "Analysis")) 
    html_text = html_text.replace("[IMAGE_PLACEHOLDER_3]", get_image_tag(k2_main, t2['title']))
    html_text = html_text.replace("[IMAGE_PLACEHOLDER_4]", get_image_tag(k2_sub + " visualization", "Market Insight"))
    return html_text

def send_email(subject, final_content):
    escaped_html = html.escape(final_content)
    email_body = f"""
    <div style="font-family: sans-serif; max-width: 800px; margin: 0 auto;">
        <h2 style="color: #2c3e50;">ìŠ¤í¬(spo) í¸ì§‘ì¥ë‹˜, ìƒˆ í¬ìŠ¤íŒ…ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ‰</h2>
        <p style="color: #e74c3c; font-weight: bold;">[í‹°ìŠ¤í† ë¦¬ ì—…ë¡œë“œìš© HTML ì½”ë“œ]</p>
        <textarea style="width: 100%; height: 200px; font-family: monospace; font-size: 13px; background-color: #f8f9fa; padding: 15px; border: 1px solid #ced4da; border-radius: 5px;" readonly>{escaped_html}</textarea>
        <hr style="border: 0; height: 1px; background: #ddd; margin: 40px 0;">
        <h3 style="color: #2c3e50;">ğŸ‘€ í¬ìŠ¤íŒ… ë¯¸ë¦¬ë³´ê¸°</h3>
        <div style="border: 1px solid #eee; padding: 30px; border-radius: 10px; background-color: #fff;">
            {final_content}
        </div>
    </div>
    """

    msg = MIMEMultipart()
    msg['From'] = GMAIL_USER
    msg['To'] = GMAIL_USER # 1ë²ˆ ì½”ë“œì²˜ëŸ¼ ë³¸ì¸ì—ê²Œ ì „ì†¡
    msg['Subject'] = subject
    msg.attach(MIMEText(email_body, 'html'))
    
    try:
        # 1ë²ˆ ì½”ë“œì˜ ì„±ê³µì ì¸ 465 SSL ë°©ì‹ ì ìš©
        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:
            server.login(GMAIL_USER, GMAIL_APP_PASSWORD)
            server.send_message(msg)
        print(f"âœ… Email Sent: {subject}")
    except Exception as e:
        print(f"âŒ Email Fail: {e}")

# --- 5. í†µí•© ì²˜ë¦¬ í•¨ìˆ˜ ---
def process_and_send(mode, category_korean, history):
    print(f"\n>>> Processing: {category_korean} ({mode})")
    candidates = get_candidates(mode)
    selected = select_top_2(candidates, history, category_korean)
    
    if len(selected) < 2:
        print(f"Not enough news for {mode}")
        return []
        
    raw_html = write_blog_post(selected[0], selected[1], category_korean)
    html_with_images = inject_images(raw_html, selected[0], selected[1])
    
    final_tistory_content = f"""
    <div class="spo-analysis-report" style="line-height: 1.8; color: #333; font-family: 'Noto Sans KR', sans-serif; word-break: keep-all; padding: 10px;">
        {html_with_images}
    </div>
    """
    
    subject = f"[{category_korean} ë¶„ì„] {selected[0]['title']} & {selected[1]['title']}"
    send_email(subject, final_tistory_content)
    
    return selected

# --- ë©”ì¸ ì‹¤í–‰ ---
def main():
    history_file = 'history.json'
    history = load_history(history_file)
    
    kst_now = datetime.datetime.now() + datetime.timedelta(hours=9)
    weekday = kst_now.weekday()
    
    new_items_total = []

    if weekday == 0: # ì›”ìš”ì¼
        items = process_and_send("TECH", "í…Œí¬", history)
        new_items_total.extend(items)
    else: # í™”~ì¼ìš”ì¼
        items_bio = process_and_send("BIO", "ë°”ì´ì˜¤", history)
        new_items_total.extend(items_bio)
        items_patent = process_and_send("PATENT", "íŠ¹í—ˆ", history)
        new_items_total.extend(items_patent)
    
    if new_items_total:
        save_history(history_file, history, new_items_total)

if __name__ == "__main__":
    main()
