import os
import json
import datetime
import time
import requests
import feedparser
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import google.generativeai as genai 
import re
import html
from bs4 import BeautifulSoup

# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ---
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
UNSPLASH_ACCESS_KEY = os.environ.get("UNSPLASH_ACCESS_KEY")
GMAIL_USER = os.environ.get("GMAIL_USER")
GMAIL_APP_PASSWORD = os.environ.get("GMAIL_APP_PASSWORD")

# Gemini ì„¤ì •
genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-3-flash-preview')

# --- 0. íˆìŠ¤í† ë¦¬ ê´€ë¦¬ ---
def load_history(filepath):
    if not os.path.exists(filepath): return []
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except: return []

def save_history(filepath, history, new_items):
    cutoff = datetime.datetime.now() - datetime.timedelta(days=30)
    cleaned = []
    
    for item in history:
        try:
            d = datetime.datetime.strptime(item.get('date', '2000-01-01'), "%Y-%m-%d")
            if d >= cutoff: cleaned.append(item)
        except: continue
        
    today = datetime.datetime.now().strftime("%Y-%m-%d")
    for item in new_items:
        cleaned.append({"id": item['id'], "title": item['title'], "date": today})
        
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(cleaned, f, ensure_ascii=False, indent=4)

# --- 1. ë°ì´í„° ìˆ˜ì§‘ (ì›¹ ìŠ¤í¬ë˜í•‘ ì¶”ê°€) ---
def scrape_article_text(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        res = requests.get(url, headers=headers, timeout=5)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'html.parser')
        paragraphs = soup.find_all('p')
        text = " ".join([p.get_text() for p in paragraphs])
        return text[:3000] if len(text) > 100 else None 
    except Exception as e:
        print(f"Scraping failed for {url}: {e}")
        return None

def fetch_rss(url, category):
    items = []
    try:
        feed = feedparser.parse(url)
        cutoff = datetime.datetime.now() - datetime.timedelta(days=7)
        for entry in feed.entries:
            if 'published_parsed' in entry and entry.published_parsed:
                pub_date = datetime.datetime.fromtimestamp(time.mktime(entry.published_parsed))
                if pub_date < cutoff: continue
            
            print(f"Scraping: {entry.title}")
            raw_text = scrape_article_text(entry.link)
            if not raw_text:
                raw_text = (entry.summary if 'summary' in entry else entry.title)[:2000]
            
            items.append({
                "id": entry.link,
                "title": entry.title,
                "type": category,
                "raw": raw_text
            })
    except Exception as e:
        print(f"RSS Error ({url}): {e}")
    return items

def get_candidates(mode):
    items = []
    if mode == "TECH":
        urls = ["https://www.theverge.com/rss/index.xml", "https://techcrunch.com/feed/"]
    elif mode == "BIO":
        urls = ["https://news.google.com/rss/search?q=Biotech+OR+%22FDA+approval%22+OR+%22Clinical+Trial%22&hl=en-US&gl=US&ceid=US:en"]
    elif mode == "PATENT":
        urls = ["https://news.google.com/rss/search?q=Patent+OR+%22Technology+Innovation%22+OR+%22Future+Tech%22&hl=en-US&gl=US&ceid=US:en"]
    
    for u in urls: items.extend(fetch_rss(u, mode))
    return items

# --- 2. ì£¼ì œ ì„ ì • ---
def select_top_2(candidates, history, category_name):
    history_ids = [h['id'] for h in history]
    filtered = [c for c in candidates if c['id'] not in history_ids]
    
    if len(filtered) < 2: return filtered[:2]
    
    cand_txt = "\n".join([f"{i}. {c['title']}" for i, c in enumerate(filtered[:15])])
    
    prompt = f"""
    ì—­í• : ì „ë¬¸ íˆ¬ì/ê¸°ìˆ  ë¸”ë¡œê·¸ í¸ì§‘ì¥ 'ìŠ¤í¬(spo)'.
    ëª©í‘œ: {category_name} ë¶„ì•¼ì—ì„œ ì‹¬ì¸µ ë¶„ì„(Deep-Dive)ì´ ê°€ëŠ¥í•˜ê³  íˆ¬ììë“¤ì˜ ê´€ì‹¬ì´ ì§‘ì¤‘ë  ë‰´ìŠ¤ 2ê°œ ì„ ì •.
    
    [í›„ë³´êµ°]
    {cand_txt}
    
    ì¡°ê±´:
    1. ê¸°ìˆ ì  ì›ë¦¬ë‚˜ ì‹œì¥ íŒŒê¸‰ë ¥ì„ ë¶„ì„í•  ê±°ë¦¬ê°€ ìˆëŠ” ì£¼ì œ ìš°ì„ .
    2. ì˜¤ì§ ìˆ«ì 2ê°œë§Œ ë°˜í™˜ (ì˜ˆ: 1, 4).
    """
    try:
        res = model.generate_content(prompt)
        nums = [int(s) for s in re.findall(r'\b\d+\b', res.text)]
        if len(nums) >= 2:
            return [filtered[nums[0]], filtered[nums[1]]]
    except: pass
    return filtered[:2]

# --- 3. ê¸€ ì‘ì„± ---
def write_blog_post(topic1, topic2, category_name):
    print(f"Writing {category_name} Post with Gemini...")
    
    # ì• ë“œì„¼ìŠ¤ ìŠ¹ì¸ ë° ê³ í’ˆì§ˆ ì½˜í…ì¸ ë¥¼ ìœ„í•œ ë¬¸ì²´ í”„ë¡¬í”„íŠ¸ ìœ ì§€
    tone_rule = """
    [êµ¬ê¸€ ì• ë“œì„¼ìŠ¤ ê³ í’ˆì§ˆ ì½˜í…ì¸ ë¥¼ ìœ„í•œ í•„ìˆ˜ ë¬¸ì²´ ë° ì–´ì¡° ì§€ì¹¨]
    1. ë¬¸ì²´: ê°€ë²¼ìš´ ë¸”ë¡œê·¸ ë§íˆ¬ë¥¼ ë°°ì œí•˜ê³ , ê²½ì œ/í…Œí¬ ì£¼ìš” ì–¸ë¡ ì‚¬ì˜ ì „ë¬¸ ì¹¼ëŸ¼ë‹ˆìŠ¤íŠ¸ì²˜ëŸ¼ ì‹ ë¢°ê° ìˆê³  ì •ì¤‘í•œ ê²½ì–´ì²´(~ìŠµë‹ˆë‹¤, ~í•©ë‹ˆë‹¤)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    2. ì „ë¬¸ì„±(E-E-A-T ì¶©ì¡±): ë‹¨ìˆœ ì‚¬ì‹¤ì˜ ë‚˜ì—´ì´ ì•„ë‹Œ, í˜„ìƒì˜ ì›ì¸ê³¼ í–¥í›„ íŒŒê¸‰ë ¥ì„ ë…¼ë¦¬ì ì´ê³  ê¹Šì´ ìˆê²Œ ë¶„ì„í•˜ì„¸ìš”. ë…ìì—ê²Œ ì‹¤ì§ˆì ì¸ ê°€ì¹˜ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.
    3. AI ë§íˆ¬ ì—„ê²© ê¸ˆì§€: 'ê²°ë¡ ì ìœ¼ë¡œ', 'ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤', 'ì´ ê¸°ì‚¬ë¥¼ í†µí•´', 'ì•ˆë…•í•˜ì„¸ìš”' ë“± AI íŠ¹ìœ ì˜ ê¸°ê³„ì ì´ê³  ìƒíˆ¬ì ì¸ ë„ì…/ë§ˆë¬´ë¦¬ í‘œí˜„ì„ ì ˆëŒ€ ê¸ˆì§€í•©ë‹ˆë‹¤. ë¬¸ë‹¨ê³¼ ë¬¸ë‹¨ ì‚¬ì´ë¥¼ ì•„ì£¼ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì£¼ì„¸ìš”.
    4. ì²´ë¥˜ ì‹œê°„ ìœ ë„: ë¬¸ì¥ì€ ê°„ê²°í•˜ë©´ì„œë„ ì •ë³´ì˜ ë°€ë„ë¥¼ ë†’ì—¬ ì‘ì„±í•˜ì—¬ ë…ìì˜ ì´íƒˆì„ ë°©ì§€í•˜ì„¸ìš”.
    """

    # [ìˆ˜ì •ëœ ë¶€ë¶„] 7ë‹¨ê³„ êµ¬ì¡°ë¡œ í™•ì¥ (ì°¨ë³„ì  ë¹„êµ ë° ê¸ì •ì  ì „ë§ ì¶”ê°€)
    structure_instruction = """
    ê° ì£¼ì œë³„ë¡œ ë°˜ë“œì‹œ ì•„ë˜ 7ê°€ì§€ H2 íƒœê·¸ ì„¹ì…˜ì„ í¬í•¨í•´ì•¼ í•¨:
    1. <h2>1. ë°°ê²½ ë° ê°œìš” (The Context)</h2> : í˜„ ìƒí™©ì„ 3ì¤„ ìš”ì•½ ë¦¬ìŠ¤íŠ¸(<ul>)ë¡œ ì œì‹œ.
    2. <h2>2. ê¸°ì¡´ ê¸°ìˆ /ì•½ë¬¼ê³¼ì˜ ì°¨ë³„ì  (Comparative Analysis)</h2> : ê³¼ê±° ìœ ì‚¬í–ˆë˜ íŠ¹í—ˆë‚˜ ê¸°ìˆ , ì•½ë¬¼ ë“±ê³¼ ë¹„êµí•˜ì—¬ ì´ë²ˆ ì£¼ì œê°€ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ í˜ì‹ ì ì¸ì§€ ëª…í™•íˆ ë¶„ì„.
    3. <h2>3. ê¸°ìˆ ì  ë©”ì»¤ë‹ˆì¦˜ (Technical Deep-Dive)</h2> : <table>ì„ 1ê°œ ì´ìƒ ë°˜ë“œì‹œ í¬í•¨.
    4. <h2>4. ì‹œì¥ íŒë„ ë° ê²½ìŸì‚¬ ë¶„ì„ (Market Dynamics)</h2> : ê°ê´€ì ì¸ [ìˆ˜ì¹˜/ë°ì´í„°] í¬í•¨.
    5. <h2>5. ë¦¬ìŠ¤í¬ ë° í•œê³„ì  (Risk Factors)</h2> : ê·œì œ, ê²½ìŸ, ê¸°ìˆ ì  ì¥ë²½ ë¶„ì„.
    6. <h2>6. ê¸ì •ì  ì „ë§ ë° ê¸°ëŒ€ íš¨ê³¼ (Future Hope & Impact)</h2> : ì´ ê¸°ìˆ /ì•½ë¬¼ì´ í–¥í›„ ê´€ë ¨ ì‚°ì—…ì´ë‚˜ ì¸ë¥˜ì—ê²Œ ê°€ì ¸ì˜¬ í¬ë§ì ì¸ ë¯¸ë˜ì™€ ê¸ì •ì  íŒŒê¸‰ë ¥ ì„œìˆ .
    7. <h2>7. ìŠ¤í¬(spo)ì˜ ì¸ì‚¬ì´íŠ¸ (Actionable Insights)</h2> : ì‹œì‚¬ì .
    """
    glossary_rule = "ì–´ë ¤ìš´ 'ì „ë¬¸ ìš©ì–´'ëŠ” ë°˜ë“œì‹œ <u> íƒœê·¸ë¡œ ê°ì‹¸ì£¼ì„¸ìš”."
    bold_rule = "ê°€ë…ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ë¬¸ë‹¨ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ 'í•µì‹¬ ë¬¸ì¥'ê³¼ 'ì£¼ìš” í‚¤ì›Œë“œ(ë‹¨ì–´)'ëŠ” ë°˜ë“œì‹œ <b> íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ êµµê²Œ ê°•ì¡°í•´ì£¼ì„¸ìš”."

    outline = model.generate_content(f"ì£¼ì œ1: {topic1['title']}\nì£¼ì œ2: {topic2['title']}\nìœ„ ë‘ ì£¼ì œë¡œ '{category_name} ì‹¬ì¸µ ë¶„ì„' ê°œìš” ì‘ì„±.").text
    
    p1_prompt = f"""
    ì—­í• : ë…ë³´ì ì¸ í†µì°°ë ¥ì„ ì§€ë‹Œ {category_name} ë¶„ì•¼ ìµœê³  ì „ë¬¸ ë¶„ì„ê°€ 'ìŠ¤í¬(spo)'.
    ê°œìš”: {outline}
    ì£¼ì œ 1: {topic1['title']} / ì›ë¬¸ ë‚´ìš©: {topic1['raw']}
    {tone_rule}
    {glossary_rule}\n{bold_rule}
    [ì‘ì„± ì§€ì¹¨] HTML íƒœê·¸ë§Œ ì¶œë ¥.
    <h1>[{category_name} ì‹¬ì¸µë¶„ì„] {topic1['title']}</h1>
    [IMAGE_PLACEHOLDER_1]
    {structure_instruction}
    [IMAGE_PLACEHOLDER_2]
    ì£¼ì œ 1ì˜ ë‚´ìš©ë§Œ ì‘ì„±.
    """
    part1 = re.sub(r"```[a-zA-Z]*\n?|```", "", model.generate_content(p1_prompt).text).strip()
    
    p2_prompt = f"""
    ì•ë¶€ë¶„: {part1}
    ì£¼ì œ 2: {topic2['title']} / ì›ë¬¸ ë‚´ìš©: {topic2['raw']}
    {tone_rule}
    {glossary_rule}\n{bold_rule}
    [ì‘ì„± ì§€ì¹¨] ì• ë‚´ìš©ê³¼ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ë„ë¡ ì‘ì„±. HTML íƒœê·¸ë§Œ ì¶œë ¥.
    <br><hr style="border: 0; height: 1px; background: #ddd; margin: 40px 0;"><br>
    <h1>[{category_name} ì‹¬ì¸µë¶„ì„] {topic2['title']}</h1>
    [IMAGE_PLACEHOLDER_3]
    {structure_instruction}
    [IMAGE_PLACEHOLDER_4]
    <br><hr style="border: 0; height: 2px; background: #2c3e50; margin: 50px 0;"><br>
    <h2>ğŸ¯ í†µí•© ì¸ì‚¬ì´íŠ¸: ë‘ ë‰´ìŠ¤ê°€ ê·¸ë¦¬ëŠ” ë¯¸ë˜ (The Bridge)</h2>
    <h2>ğŸ“– ì˜¤ëŠ˜ì˜ ìš©ì–´ ì •ë¦¬ (Glossary)</h2>
    <h2>ğŸ” SEO ë° íƒœê·¸ ì •ë³´ (ì—…ë¡œë“œìš©)</h2>
    <hr style="border: 0; height: 1px; background: #eee; margin: 40px 0;">
    <p style="color:grey; font-size: 0.9em; text-align: center;">* ë³¸ ì½˜í…ì¸ ëŠ” ì •ë³´ ì œê³µì„ ëª©ì ìœ¼ë¡œ í•˜ë©°, íˆ¬ìì˜ ì±…ì„ì€ ë³¸ì¸ì—ê²Œ ìˆìŠµë‹ˆë‹¤. <br> Editor: ìŠ¤í¬(spo)</p>
    """
    part2 = re.sub(r"```[a-zA-Z]*\n?|```", "", model.generate_content(p2_prompt).text).strip()
    
    return part1 + "\n" + part2

# --- 4. ì´ë¯¸ì§€, ëª©ì°¨ ìƒì„± ë° ì´ë©”ì¼ ì „ì†¡ ---
def get_image_tag(keyword, alt_text=""):
    search_query = f"{keyword}"
    url = f"https://api.unsplash.com/search/photos?query={search_query}&per_page=1&orientation=landscape&client_id={UNSPLASH_ACCESS_KEY}"
    try:
        data = requests.get(url, timeout=5).json()
        if not data['results']: 
            return ""
        img_url = data['results'][0]['urls']['regular']
        return f"""
        <figure style="margin: 30px 0;">
            <img src='{img_url}' alt='{alt_text}' style='width:100%; border-radius:12px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);'>
            <figcaption style='color:#666; font-size:13px; text-align:center; margin-top:10px;'>Source: Unsplash ({keyword})</figcaption>
        </figure>
        """
    except: return ""

def inject_images(html_text, t1, t2):
    prompt = "Unsplash ì´ë¯¸ì§€ ê²€ìƒ‰ìš© ì˜ë¬¸ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì¤˜. ë³µì¡í•œ ê³ ìœ ëª…ì‚¬, ì‹ ì•½ ì´ë¦„, íŠ¹í—ˆ ë²ˆí˜¸ ë“±ì€ ëª¨ë‘ ë°°ì œí•˜ê³  'laboratory', 'doctor', 'medicine', 'blueprint', 'technology innovation' ê°™ì´ ë§¤ìš° ì§ê´€ì ì´ê³  ì‹œê°ì ì¸ ë²”ìš© ë‹¨ì–´ 1~2ê°œë§Œ ì¶œë ¥í•´. ê¸°í˜¸ ì—†ì´ ì˜ë¬¸ë§Œ ì¶œë ¥:\n{}"
    try:
        k1_main = re.sub(r'[^a-zA-Z0-9\s]', '', model.generate_content(prompt.format(t1['title'])).text.strip())
        k1_sub = re.sub(r'[^a-zA-Z0-9\s]', '', model.generate_content(prompt.format(t1['title'] + " (Focus on abstract business or data concept)")).text.strip())
        k2_main = re.sub(r'[^a-zA-Z0-9\s]', '', model.generate_content(prompt.format(t2['title'])).text.strip())
        k2_sub = re.sub(r'[^a-zA-Z0-9\s]', '', model.generate_content(prompt.format(t2['title'] + " (Focus on abstract business or data concept)")).text.strip())
    except: 
        k1_main, k1_sub = "science research", "data analysis"
        k2_main, k2_sub = "future technology", "business innovation"
    
    html_text = html_text.replace("[IMAGE_PLACEHOLDER_1]", get_image_tag(k1_main, t1['title']))
    html_text = html_text.replace("[IMAGE_PLACEHOLDER_2]", get_image_tag(k1_sub, "Analysis")) 
    html_text = html_text.replace("[IMAGE_PLACEHOLDER_3]", get_image_tag(k2_main, t2['title']))
    html_text = html_text.replace("[IMAGE_PLACEHOLDER_4]", get_image_tag(k2_sub, "Market Insight"))
    return html_text

def generate_toc_and_add_ids(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    
    toc_html = "<div class='spo-toc' style='background-color: #f8f9fa; padding: 25px; border-radius: 12px; margin-bottom: 40px; border: 1px solid #e9ecef;'>\n"
    toc_html += "<h2 style='margin-top: 0; color: #2c3e50; font-size: 1.4em; border-bottom: 2px solid #3498db; padding-bottom: 10px; display: inline-block;'>ğŸ“‘ ëª©ì°¨</h2>\n"
    toc_html += "<ul style='list-style-type: none; padding-left: 0; margin-bottom: 0; line-height: 1.8;'>\n"
    
    headings = soup.find_all(['h1', 'h2'])
    for idx, tag in enumerate(headings):
        anchor_id = f"section-{idx}"
        tag['id'] = anchor_id
        text = tag.get_text(strip=True)
        
        if tag.name == 'h1':
            toc_html += f"<li style='margin-top: 15px; font-weight: bold; font-size: 1.1em;'><a href='#{anchor_id}' style='color: #2980b9; text-decoration: none;'>{text}</a></li>\n"
        elif tag.name == 'h2':
            toc_html += f"<li style='margin-top: 5px; margin-left: 20px; font-size: 0.95em;'><a href='#{anchor_id}' style='color: #34495e; text-decoration: none;'>- {text}</a></li>\n"
            
    toc_html += "</ul>\n</div>\n"
    return toc_html + str(soup)

def apply_namuwiki_tooltips(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    
    glossary_dict = {}
    glossary_header = soup.find(lambda tag: tag.name == 'h2' and 'ìš©ì–´ ì •ë¦¬' in tag.text)
    
    if glossary_header:
        list_tag = glossary_header.find_next_sibling(['ul', 'ol', 'dl'])
        if list_tag:
            for item in list_tag.find_all('li'):
                text = item.get_text(strip=True)
                if ':' in text:
                    parts = text.split(':', 1)
                elif '-' in text:
                    parts = text.split('-', 1)
                else:
                    continue
                    
                if len(parts) == 2:
                    term = parts[0].strip()
                    desc = parts[1].strip()
                    glossary_dict[term] = desc

    tooltip_css = """
    <style>
    .spo-tooltip-container {
        position: relative;
        display: inline-block;
        border-bottom: 2px dashed #3498db;
        color: #2980b9;
        cursor: pointer;
        font-weight: bold;
        text-decoration: none !important;
    }
    .spo-tooltip-container .spo-tooltip-text {
        visibility: hidden;
        width: max-content;
        max-width: 320px;
        background-color: #2c3e50;
        color: #ffffff;
        text-align: left;
        border-radius: 8px;
        padding: 10px 14px;
        position: absolute;
        z-index: 9999;
        bottom: 130%;
        left: 50%;
        transform: translateX(-50%);
        opacity: 0;
        transition: opacity 0.3s ease, transform 0.3s ease;
        font-size: 14px;
        font-weight: normal;
        line-height: 1.6;
        box-shadow: 0 4px 15px rgba(0,0,0,0.2);
        word-break: keep-all;
        white-space: pre-wrap;
    }
    .spo-tooltip-container .spo-tooltip-text::after {
        content: "";
        position: absolute;
        top: 100%;
        left: 50%;
        margin-left: -6px;
        border-width: 6px;
        border-style: solid;
        border-color: #2c3e50 transparent transparent transparent;
    }
    .spo-tooltip-container:hover .spo-tooltip-text,
    .spo-tooltip-container:active .spo-tooltip-text {
        visibility: visible;
        opacity: 1;
        transform: translateX(-50%) translateY(-3px);
    }
    </style>
    """

    if glossary_dict:
        for u_tag in soup.find_all('u'):
            term_text = u_tag.get_text(strip=True)
            
            matched_desc = None
            for key, desc in glossary_dict.items():
                if term_text.lower() in key.lower() or key.lower() in term_text.lower():
                    matched_desc = desc
                    break
            
            if matched_desc:
                span_container = soup.new_tag("span", attrs={"class": "spo-tooltip-container"})
                span_container.string = term_text
                
                span_tooltip = soup.new_tag("span", attrs={"class": "spo-tooltip-text"})
                span_tooltip.string = matched_desc
                
                span_container.append(span_tooltip)
                u_tag.replace_with(span_container)
                
    return tooltip_css + str(soup)

def send_email(subject, final_content):
    escaped_html = html.escape(final_content)
    email_body = f"""
    <div style="font-family: sans-serif; max-width: 800px; margin: 0 auto;">
        <h2 style="color: #2c3e50;">ìŠ¤í¬(spo) í¸ì§‘ì¥ë‹˜, ìƒˆ í¬ìŠ¤íŒ…ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ‰</h2>
        <p style="color: #e74c3c; font-weight: bold;">[í‹°ìŠ¤í† ë¦¬ ì—…ë¡œë“œìš© HTML ì½”ë“œ]</p>
        <textarea style="width: 100%; height: 200px; font-family: monospace; font-size: 13px; background-color: #f8f9fa; padding: 15px; border: 1px solid #ced4da; border-radius: 5px;" readonly>{escaped_html}</textarea>
        <hr style="border: 0; height: 1px; background: #ddd; margin: 40px 0;">
        <h3 style="color: #2c3e50;">ğŸ‘€ í¬ìŠ¤íŒ… ë¯¸ë¦¬ë³´ê¸°</h3>
        <div style="border: 1px solid #eee; padding: 30px; border-radius: 10px; background-color: #fff;">
            {final_content}
        </div>
    </div>
    """

    msg = MIMEMultipart()
    msg['From'] = GMAIL_USER
    msg['To'] = GMAIL_USER 
    msg['Subject'] = subject
    msg.attach(MIMEText(email_body, 'html'))
    
    try:
        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:
            server.login(GMAIL_USER, GMAIL_APP_PASSWORD)
            server.send_message(msg)
        print(f"âœ… Email Sent: {subject}")
    except Exception as e:
        print(f"âŒ Email Fail: {e}")

# --- 5. í†µí•© ì²˜ë¦¬ í•¨ìˆ˜ ---
def process_and_send(mode, category_korean, history):
    print(f"\n>>> Processing: {category_korean} ({mode})")
    candidates = get_candidates(mode)
    selected = select_top_2(candidates, history, category_korean)
    
    if len(selected) < 2:
        print(f"Not enough news for {mode}")
        return []
        
    raw_html = write_blog_post(selected[0], selected[1], category_korean)
    html_with_images = inject_images(raw_html, selected[0], selected[1])
    
    html_with_toc = generate_toc_and_add_ids(html_with_images)
    
    html_with_tooltips = apply_namuwiki_tooltips(html_with_toc)
    
    final_tistory_content = f"""
    <div class="spo-analysis-report" style="line-height: 1.8; color: #333; font-family: 'Noto Sans KR', sans-serif; word-break: keep-all; padding: 10px;">
        {html_with_tooltips}
    </div>
    """
    
    try:
        t1_kr = model.generate_content(f"ë‹¤ìŒ ì˜ë¬¸ ì œëª©ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¸”ë¡œê·¸ ì œëª©ìœ¼ë¡œ ë²ˆì—­í•´. ë‹¤ë¥¸ ë§ ì—†ì´ ê²°ê³¼ë§Œ ì¶œë ¥í•´:\n{selected[0]['title']}").text.strip()
        t2_kr = model.generate_content(f"ë‹¤ìŒ ì˜ë¬¸ ì œëª©ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¸”ë¡œê·¸ ì œëª©ìœ¼ë¡œ ë²ˆì—­í•´. ë‹¤ë¥¸ ë§ ì—†ì´ ê²°ê³¼ë§Œ ì¶œë ¥í•´:\n{selected[1]['title']}").text.strip()
    except:
        t1_kr, t2_kr = selected[0]['title'], selected[1]['title']
    
    subject = f"[{category_korean} ë¶„ì„] {t1_kr} & {t2_kr}"
    send_email(subject, final_tistory_content)
    
    return selected

# --- ë©”ì¸ ì‹¤í–‰ ---
def main():
    history_file = 'history.json'
    history = load_history(history_file)
    
    kst_now = datetime.datetime.now() + datetime.timedelta(hours=9)
    weekday = kst_now.weekday()
    
    new_items_total = []

    if weekday == 0: # ì›”ìš”ì¼
        items = process_and_send("TECH", "í…Œí¬", history)
        new_items_total.extend(items)
    else: # í™”~ì¼ìš”ì¼
        items_bio = process_and_send("BIO", "ë°”ì´ì˜¤", history)
        new_items_total.extend(items_bio)
        items_patent = process_and_send("PATENT", "íŠ¹í—ˆ", history)
        new_items_total.extend(items_patent)
    
    if new_items_total:
        save_history(history_file, history, new_items_total)

if __name__ == "__main__":
    main()
