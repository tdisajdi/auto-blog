import os
import json
import datetime
import time
import requests
import feedparser
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import google.generativeai as genai 
import re
import html
from bs4 import BeautifulSoup

# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ---
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
UNSPLASH_ACCESS_KEY = os.environ.get("UNSPLASH_ACCESS_KEY")
GMAIL_USER = os.environ.get("GMAIL_USER")
GMAIL_APP_PASSWORD = os.environ.get("GMAIL_APP_PASSWORD")

# Gemini ì„¤ì •
genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-3-flash-preview')

# --- 0. íˆìŠ¤í† ë¦¬ ê´€ë¦¬ ---
def load_history(filepath):
    if not os.path.exists(filepath): return []
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except: return []

def save_history(filepath, history, new_items):
    cutoff = datetime.datetime.now() - datetime.timedelta(days=30)
    cleaned = []
    
    for item in history:
        try:
            d = datetime.datetime.strptime(item.get('date', '2000-01-01'), "%Y-%m-%d")
            if d >= cutoff: cleaned.append(item)
        except: continue
        
    today = datetime.datetime.now().strftime("%Y-%m-%d")
    for item in new_items:
        cleaned.append({"id": item['id'], "title": item['title'], "date": today})
        
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(cleaned, f, ensure_ascii=False, indent=4)

# --- 1. ë°ì´í„° ìˆ˜ì§‘ (ì›¹ ìŠ¤í¬ë˜í•‘ ì¶”ê°€) ---
def scrape_article_text(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        res = requests.get(url, headers=headers, timeout=5)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'html.parser')
        paragraphs = soup.find_all('p')
        text = " ".join([p.get_text() for p in paragraphs])
        return text[:3000] if len(text) > 100 else None 
    except Exception as e:
        print(f"Scraping failed for {url}: {e}")
        return None

def fetch_rss(url, category):
    items = []
    try:
        feed = feedparser.parse(url)
        cutoff = datetime.datetime.now() - datetime.timedelta(days=3)
        for entry in feed.entries:
            if 'published_parsed' in entry and entry.published_parsed:
                pub_date = datetime.datetime.fromtimestamp(time.mktime(entry.published_parsed))
                if pub_date < cutoff: continue
            
            print(f"Scraping: {entry.title}")
            raw_text = scrape_article_text(entry.link)
            if not raw_text:
                raw_text = (entry.summary if 'summary' in entry else entry.title)[:2000]
            
            items.append({
                "id": entry.link,
                "title": entry.title,
                "type": category,
                "raw": raw_text
            })
    except Exception as e:
        print(f"RSS Error ({url}): {e}")
    return items

def get_candidates(mode):
    items = []
    if mode == "TECH":
        urls = ["https://www.theverge.com/rss/index.xml", "https://techcrunch.com/feed/"]
    elif mode == "BIO":
        urls = ["https://news.google.com/rss/search?q=Biotech+OR+%22FDA+approval%22+OR+%22Clinical+Trial%22&hl=en-US&gl=US&ceid=US:en"]
    elif mode == "PATENT":
        urls = ["https://news.google.com/rss/search?q=Patent+OR+%22Technology+Innovation%22+OR+%22Future+Tech%22&hl=en-US&gl=US&ceid=US:en"]
    
    for u in urls: items.extend(fetch_rss(u, mode))
    return items

# --- 2. ì£¼ì œ ì„ ì • ---
def select_top_2(candidates, history, category_name):
    history_ids = [h['id'] for h in history]
    filtered = [c for c in candidates if c['id'] not in history_ids]
    
    if len(filtered) < 2: return filtered[:2]
    
    cand_txt = "\n".join([f"{i}. {c['title']}" for i, c in enumerate(filtered[:15])])
    
    prompt = f"""
    ì—­í• : ì „ë¬¸ íˆ¬ì/ê¸°ìˆ  ë¸”ë¡œê·¸ í¸ì§‘ì¥ 'ìŠ¤í¬(spo)'.
    ëª©í‘œ: {category_name} ë¶„ì•¼ì—ì„œ ì‹¬ì¸µ ë¶„ì„(Deep-Dive)ì´ ê°€ëŠ¥í•˜ê³  íˆ¬ììë“¤ì˜ ê´€ì‹¬ì´ ì§‘ì¤‘ë  ë‰´ìŠ¤ 2ê°œ ì„ ì •.
    
    [í›„ë³´êµ°]
    {cand_txt}
    
    ì¡°ê±´:
    1. ê¸°ìˆ ì  ì›ë¦¬ë‚˜ ì‹œì¥ íŒŒê¸‰ë ¥ì„ ë¶„ì„í•  ê±°ë¦¬ê°€ ìˆëŠ” ì£¼ì œ ìš°ì„ .
    2. ì˜¤ì§ ìˆ«ì 2ê°œë§Œ ë°˜í™˜ (ì˜ˆ: 1, 4).
    """
    try:
        res = model.generate_content(prompt)
        time.sleep(5) # API í˜¸ì¶œ ì œí•œ ë°©ì§€
        nums = [int(s) for s in re.findall(r'\b\d+\b', res.text)]
        if len(nums) >= 2:
            return [filtered[nums[0]], filtered[nums[1]]]
    except: pass
    return filtered[:2]

# --- 3. ë§¤ë ¥ì ì¸ í•œêµ­ì–´ ì œëª© ìƒì„± í•¨ìˆ˜ (ìˆ˜ì •ë¨) ---
def get_catchy_korean_title(english_title):
    prompt = f"""
    ë‹¤ìŒ ì˜ë¬¸ ë‰´ìŠ¤ ì œëª©ì„ ë²ˆì—­í•˜ë˜, ì‚¬ëŒë“¤ì˜ í˜¸ê¸°ì‹¬ì„ ëŒë©´ì„œë„ 'ê¹”ë”í•˜ê³  í•µì‹¬ì„ ì°Œë¥´ëŠ”' í•œ ì¤„ì§œë¦¬ í•œêµ­ì–´ ë¸”ë¡œê·¸ ì œëª©ìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜.
    
    [ì¡°ê±´]
    1. ë¬´ì¡°ê±´ 100% í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•  ê²ƒ (ë¶ˆê°€í”¼í•œ ê³ ìœ ëª…ì‚¬ ì œì™¸).
    2. ê¸¸ì´ëŠ” 30ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ 1ì¤„ë¡œ ì‘ì„±í•  ê²ƒ.
    3. ê³¼ë„í•œ íŠ¹ìˆ˜ê¸°í˜¸(!, ?, [])ë‚˜ ì–´ê·¸ë¡œì„± ë‹¨ì–´([ì¶©ê²©], [ê²½ì•…] ë“±)ëŠ” í”¼í•˜ê³ , ì „ë¬¸ê°€ë‹¤ìš´ ì„¸ë ¨ë˜ê³  íŠ¸ë Œë””í•œ ëŠë‚Œì„ ì¤„ ê²ƒ.
    4. ë‹¤ë¥¸ ë¶€ê°€ ì„¤ëª… ì—†ì´ ì˜¤ì§ ìƒì„±ëœ 'ì œëª© 1ê°œ'ë§Œ ì¶œë ¥í•  ê²ƒ.
    
    ì˜ë¬¸ ì œëª©: {english_title}
    """
    try:
        title_res = model.generate_content(prompt).text.strip()
        time.sleep(5) # API í˜¸ì¶œ ì œí•œ ë°©ì§€
        return title_res
    except:
        return english_title

# --- ì´ë©”ì¼ ì „ì†¡ìš© í†µí•© ì œëª© ìƒì„± í•¨ìˆ˜ (ì¶”ê°€ë¨) ---
def get_unified_subject(category_name, t1_kr, t2_kr):
    prompt = f"""
    ë‹¤ìŒ ë‘ ê°œì˜ ë‰´ìŠ¤ ì œëª©ì„ ì•„ìš°ë¥´ëŠ”, ì´ë©”ì¼ ì œëª©ìš© í†µí•© ë¸”ë¡œê·¸ ì œëª©ì„ ì‘ì„±í•´ì¤˜.

    [ì¡°ê±´]
    1. ë§¤ìš° ê°„ê²°í•˜ê³  ê¹”ë”í•˜ê²Œ 1ì¤„ë¡œ ì‘ì„±í•  ê²ƒ (ìµœëŒ€ 35ì ì´ë‚´).
    2. ë‘ ì£¼ì œì˜ í•µì‹¬ ë‚´ìš©ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì–´ìš°ëŸ¬ì§€ëŠ” í•˜ë‚˜ì˜ ë¬¸ì¥ì´ë‚˜ êµ¬ë¡œ ë§Œë“¤ ê²ƒ.
    3. '&' ê°™ì€ ê¸°í˜¸ë‚˜ ìê·¹ì ì¸ ìˆ˜ì‹ì–´ë¥¼ ë¹¼ê³  ë‹´ë°±í•˜ë©´ì„œë„ ì¸ì‚¬ì´íŠ¸ê°€ ëŠê»´ì§€ë„ë¡ í•  ê²ƒ.
    4. ë‹¤ë¥¸ ë¶€ê°€ ì„¤ëª… ì—†ì´ ì˜¤ì§ ìƒì„±ëœ 'ì œëª© 1ê°œ'ë§Œ ì¶œë ¥í•  ê²ƒ.

    ì£¼ì œ1: {t1_kr}
    ì£¼ì œ2: {t2_kr}
    """
    try:
        res = model.generate_content(prompt).text.strip()
        time.sleep(5) # API í˜¸ì¶œ ì œí•œ ë°©ì§€
        return f"[{category_name} ë¶„ì„] {res}"
    except:
        return f"[{category_name} ë¶„ì„] {t1_kr[:15]}... ì™¸ í•µì‹¬ ì´ìŠˆ"

# --- 4. ê¸€ ì‘ì„± ---
def write_blog_post(topic1, topic2, category_name, t1_kr, t2_kr):
    print(f"Writing {category_name} Post with Gemini...")
    
    tone_rule = """
    [êµ¬ê¸€ ì• ë“œì„¼ìŠ¤ ìŠ¹ì¸ ë° ê³ í’ˆì§ˆ ì½˜í…ì¸ ë¥¼ ìœ„í•œ í•„ìˆ˜ ë¬¸ì²´ ë° ì–´ì¡° ì§€ì¹¨ (10ë…„ì°¨ í˜„ì—… ì „ë¬¸ê°€ í˜ë¥´ì†Œë‚˜)]
    1. í˜ë¥´ì†Œë‚˜: ê´€ë ¨ ì—…ê³„ì—ì„œ 10ë…„ ì´ìƒ êµ¬ë¥´ë©°(?) ì‚°ì „ìˆ˜ì „ ë‹¤ ê²ªì€ ë² í…Œë‘ ì‹¤ë¬´ ì „ë¬¸ê°€. ì§€ë£¨í•œ êµê³¼ì„œì  ì„¤ëª…ì´ ì•„ë‹Œ, í˜„ì—…ì˜ 'ì§„ì§œ ëŒì•„ê°€ëŠ” ì´ì•¼ê¸°'ë¥¼ íŠ¸ë Œë””í•˜ê³  ê°ê°ì ìœ¼ë¡œ í’€ì–´ëƒ…ë‹ˆë‹¤.
    2. ë¬¸ì²´: êµ°ë”ë”ê¸° ì—†ì´ ê¹”ë”í•˜ê³  ê°€ë…ì„± ë†’ì€ ë¬¸ì¥ì„ êµ¬ì‚¬í•©ë‹ˆë‹¤. "~ìŠµë‹ˆë‹¤", "~í•˜ì£ " ë“± ì‹ ë¢°ê° ìˆëŠ” ê²½ì–´ì²´ë¥¼ ì‚¬ìš©í•˜ë˜, ì§€ì¸ì—ê²Œ ê³ ê¸‰ ì‹¤ë¬´ ì •ë³´ë¥¼ ìŠ¬ì© ê³µìœ í•´ì£¼ë“¯ ìì—°ìŠ¤ëŸ½ê³  ì „ë¬¸ê°€ íŠ¹ìœ ì˜ ì—¬ìœ ê°€ ë¬»ì–´ë‚˜ëŠ” ì–´íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    3. ì£¼ê´€ì ì´ê³  ì˜ˆë¦¬í•œ ë¶„ì„: ë‹¨ìˆœ ì‚¬ì‹¤ ì „ë‹¬ì„ ë„˜ì–´, "ì†”ì§íˆ ì´ë²ˆ ì´ìŠˆë¡œ ë³¼ ë•Œ Aì‚¬ë³´ë‹¤ëŠ” Bì‚¬ê°€ ì‹œì¥ ì„ ì ì— í›¨ì”¬ ìœ ë¦¬í•œ ê³ ì§€ë¥¼ ì°¨ì§€í•  ê²ë‹ˆë‹¤. í˜„ì—…ì—ì„œ ê·¸ë ‡ê²Œ ë³´ëŠ” ì´ìœ ëŠ”..."ê³¼ ê°™ì´ 10ë…„ì°¨ íŠ¹ìœ ì˜ ëšœë ·í•œ ì£¼ê´€ê³¼ ì˜ˆë¦¬í•œ ë¹„êµ ë¶„ì„ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.
    4. AI ë§íˆ¬ 200% ê¸ˆì§€: 'ê²°ë¡ ì ìœ¼ë¡œ', 'ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤', 'ì´ ê¸°ì‚¬ë¥¼ í†µí•´', 'ì•ˆë…•í•˜ì„¸ìš”', 'ìš”ì•½í•˜ìë©´', 'í¥ë¯¸ì§„ì§„í•œ' ë“± AI íŠ¹ìœ ì˜ ìƒíˆ¬ì ì´ê³  ì˜í˜¼ ì—†ëŠ” í‘œí˜„ì€ ì ˆëŒ€ ê¸ˆì§€í•©ë‹ˆë‹¤. ì§„ì§œ ì‚¬ëŒì´ ì“´ ê²ƒì²˜ëŸ¼ ë¬¸ë‹¨ ê°„ ì—°ê²°ì„ ë§¤ë„ëŸ½ê²Œ í•˜ì„¸ìš”.
    """

    structure_instruction = """
    ê° ì£¼ì œë³„ë¡œ ë°˜ë“œì‹œ ì•„ë˜ 7ê°€ì§€ H2 íƒœê·¸ ì„¹ì…˜ì„ í¬í•¨í•´ì•¼ í•¨:
    1. <h2>1. ë°°ê²½ ë° ê°œìš” (The Context)</h2> : í˜„ ìƒí™©ì„ ë»”í•˜ì§€ ì•Šê²Œ 3ì¤„ ìš”ì•½ ë¦¬ìŠ¤íŠ¸(<ul>)ë¡œ ì œì‹œ.
    2. <h2>2. ê¸°ì¡´ ê¸°ìˆ /ì•½ë¬¼ê³¼ì˜ ì°¨ë³„ì  (Comparative Analysis)</h2> : ê³¼ê±° ìœ ì‚¬í–ˆë˜ ì‚¬ë¡€ì™€ ë¹„êµí•˜ì—¬ ì´ë²ˆ ì£¼ì œì˜ ì§„ì§œ í˜ì‹  í¬ì¸íŠ¸ê°€ ë¬´ì—‡ì¸ì§€ ì—ë””í„°ì˜ ì‹œê°ìœ¼ë¡œ ë¶„ì„.
    3. <h2>3. ê¸°ìˆ ì  ë©”ì»¤ë‹ˆì¦˜ (Technical Deep-Dive)</h2> : <table>ì„ 1ê°œ ì´ìƒ ë°˜ë“œì‹œ í¬í•¨. ì „ë¬¸ì ì´ì§€ë§Œ ë…ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì ì ˆí•œ ë¹„ìœ ë¥¼ ì„ì–´ ì„¤ëª….
    4. <h2>4. ì‹œì¥ íŒë„ ë° ê²½ìŸì‚¬ ë¶„ì„ (Market Dynamics)</h2> : [ë§¤ìš° ì¤‘ìš”] ê°ê´€ì ì¸ ë°ì´í„°ì™€ í•¨ê»˜, "A ê¸°ì—…ë³´ë‹¤ B ê¸°ì—…ì´ ì´ êµ­ë©´ì—ì„œ ì™œ ë” ìœ ë¦¬í•œì§€", í˜¹ì€ "ê¸°ì¡´ ê°•ì C ê¸°ì—…ì—ê²Œ ì–´ë–¤ ì¹˜ëª…ì ì¸ ìœ„í˜‘ì´ ë ì§€" ë“± êµ¬ì²´ì ì´ê³  ì£¼ê´€ì ì¸ ê¸°ì—…/ê¸°ìˆ  ê°„ ìš°ìœ„ ë¶„ì„ì„ ë°˜ë“œì‹œ ì‘ì„±.
    5. <h2>5. ë¦¬ìŠ¤í¬ ë° í•œê³„ì  (Risk Factors)</h2> : í‘œë©´ì ì¸ ë¦¬ìŠ¤í¬ê°€ ì•„ë‹Œ, ì‹¤ë¬´ì/íˆ¬ìì ê´€ì ì—ì„œì˜ ì§„ì§œ ê±¸ë¦¼ëŒ(ê·œì œ, ê²½ìŸ ì‹¬í™”, ê¸°ìˆ ì  ì¥ë²½ ë“±)ì„ ì˜ˆë¦¬í•˜ê²Œ ì§€ì .
    6. <h2>6. ê¸ì •ì  ì „ë§ ë° ê¸°ëŒ€ íš¨ê³¼ (Future Hope & Impact)</h2> : ì´ ë³€í™”ê°€ ê°€ì ¸ì˜¬ ë¯¸ë˜ ì‚°ì—…ì˜ ëª¨ìŠµì„ ìƒìƒí•˜ê²Œ ê·¸ë ¤ì£¼ë“¯ ì„œìˆ .
    7. <h2>7. ìŠ¤í¬(spo)ì˜ ì¸ì‚¬ì´íŠ¸ (Actionable Insights)</h2> : ë‹¨ìˆœ ìš”ì•½ ê¸ˆì§€. "ê·¸ë˜ì„œ ì§€ê¸ˆ ìš°ë¦¬ëŠ” ë¬´ì—‡ì„ ì£¼ëª©í•´ì•¼ í•˜ëŠ”ê°€?"ì— ëŒ€í•œ ì—ë””í„° ìŠ¤í¬ì˜ ë§¤ìš° ì£¼ê´€ì ì´ê³  ì‚¬ëŒ ëƒ„ìƒˆ ë‚˜ëŠ” ì†”ì§í•œ ì´í‰ê³¼ íˆ¬ì/ì‚°ì—…ì  ì¡°ì–¸.
    """
    glossary_rule = "ì–´ë ¤ìš´ 'ì „ë¬¸ ìš©ì–´'ëŠ” ë°˜ë“œì‹œ <u> íƒœê·¸ë¡œ ê°ì‹¸ì£¼ì„¸ìš”."
    bold_rule = "ê°€ë…ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ë¬¸ë‹¨ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ 'í•µì‹¬ ë¬¸ì¥'ê³¼ 'ì£¼ìš” í‚¤ì›Œë“œ(ë‹¨ì–´)'ëŠ” ë°˜ë“œì‹œ <b> íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ êµµê²Œ ê°•ì¡°í•´ì£¼ì„¸ìš”."

    outline = model.generate_content(f"ì£¼ì œ1: {topic1['title']}\nì£¼ì œ2: {topic2['title']}\nìœ„ ë‘ ì£¼ì œë¡œ '{category_name} ì‹¬ì¸µ ë¶„ì„' ê°œìš” ì‘ì„±.").text
    time.sleep(5) # API í˜¸ì¶œ ì œí•œ ë°©ì§€
    
    p1_prompt = f"""
    ì—­í• : {category_name} ì—…ê³„ 10ë…„ì°¨ í˜„ì—… ì „ë¬¸ê°€ì´ì, íŠ¸ë Œë””í•˜ê³  ê¹”ë”í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ëŠ” ì‹¤ë¬´ ë¶„ì„ê°€ 'ìŠ¤í¬(spo)'.
    ê°œìš”: {outline}
    ì£¼ì œ 1: {topic1['title']} / ì›ë¬¸ ë‚´ìš©: {topic1['raw']}
    {tone_rule}
    {glossary_rule}\n{bold_rule}
    [ì‘ì„± ì§€ì¹¨] HTML íƒœê·¸ë§Œ ì¶œë ¥.
    <h1>[{category_name} ì‹¬ì¸µë¶„ì„] {t1_kr}</h1>
    [IMAGE_PLACEHOLDER_1]
    {structure_instruction}
    [IMAGE_PLACEHOLDER_2]
    <br>
    [IMAGE_PLACEHOLDER_3]
    ì£¼ì œ 1ì˜ ë‚´ìš©ë§Œ ì‘ì„±.
    """
    part1_res = model.generate_content(p1_prompt).text
    time.sleep(5) # API í˜¸ì¶œ ì œí•œ ë°©ì§€
    part1 = re.sub(r"```[a-zA-Z]*\n?|```", "", part1_res).strip()
    
    p2_prompt = f"""
    ì•ë¶€ë¶„: {part1}
    ì£¼ì œ 2: {topic2['title']} / ì›ë¬¸ ë‚´ìš©: {topic2['raw']}
    {tone_rule}
    {glossary_rule}\n{bold_rule}
    [ì‘ì„± ì§€ì¹¨] ì• ë‚´ìš©ê³¼ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ë„ë¡ ì‘ì„±. HTML íƒœê·¸ë§Œ ì¶œë ¥.
    <br><hr style="border: 0; height: 1px; background: #ddd; margin: 40px 0;"><br>
    <h1>[{category_name} ì‹¬ì¸µë¶„ì„] {t2_kr}</h1>
    [IMAGE_PLACEHOLDER_4]
    {structure_instruction}
    [IMAGE_PLACEHOLDER_5]
    <br>
    [IMAGE_PLACEHOLDER_6]
    <br><hr style="border: 0; height: 2px; background: #2c3e50; margin: 50px 0;"><br>
    <h2>ğŸ¯ í†µí•© ì¸ì‚¬ì´íŠ¸: ë‘ ë‰´ìŠ¤ê°€ ê·¸ë¦¬ëŠ” ë¯¸ë˜ (The Bridge)</h2>
    <h2>ğŸ“– ì˜¤ëŠ˜ì˜ ìš©ì–´ ì •ë¦¬ (Glossary)</h2>
    <h2>ğŸ” SEO ë° íƒœê·¸ ì •ë³´ (ì—…ë¡œë“œìš©)</h2>
    <hr style="border: 0; height: 1px; background: #eee; margin: 40px 0;">
    <p style="color:grey; font-size: 0.9em; text-align: center;">* ë³¸ ì½˜í…ì¸ ëŠ” ì •ë³´ ì œê³µì„ ëª©ì ìœ¼ë¡œ í•˜ë©°, íˆ¬ìì˜ ì±…ì„ì€ ë³¸ì¸ì—ê²Œ ìˆìŠµë‹ˆë‹¤. <br> Editor: ìŠ¤í¬(spo)</p>
    """
    part2_res = model.generate_content(p2_prompt).text
    time.sleep(5) # API í˜¸ì¶œ ì œí•œ ë°©ì§€
    part2 = re.sub(r"```[a-zA-Z]*\n?|```", "", part2_res).strip()
    
    return part1 + "\n" + part2

# --- 5. ì´ë¯¸ì§€, ëª©ì°¨ ìƒì„± ë° ì´ë©”ì¼ ì „ì†¡ ---
def get_image_tag(keyword, used_urls, alt_text=""):
    search_query = f"{keyword}"
    # per_pageë¥¼ 5ë¡œ ëŠ˜ë ¤ ì¤‘ë³µì„ ê²€ì‚¬í•  í›„ë³´êµ°ì„ í™•ë³´í•©ë‹ˆë‹¤.
    url = f"https://api.unsplash.com/search/photos?query={search_query}&per_page=5&orientation=landscape&client_id={UNSPLASH_ACCESS_KEY}"
    try:
        data = requests.get(url, timeout=5).json()
        if not data.get('results'): 
            return ""
        
        img_url = ""
        # ë¶ˆëŸ¬ì˜¨ ê²°ê³¼ ì¤‘ ì‚¬ìš©ëœ ì  ì—†ëŠ” ì´ë¯¸ì§€ URLì„ ì°¾ìŠµë‹ˆë‹¤.
        for res in data['results']:
            candidate_url = res['urls']['regular']
            if candidate_url not in used_urls:
                img_url = candidate_url
                used_urls.add(img_url)
                break
        
        # ë§Œì•½ ì „ë¶€ ì¤‘ë³µì´ë¼ë©´ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¥¼ ì–´ì©” ìˆ˜ ì—†ì´ ì‚¬ìš©í•©ë‹ˆë‹¤.
        if not img_url:
            img_url = data['results'][0]['urls']['regular']
            used_urls.add(img_url)

        return f"""
        <figure style="margin: 30px 0;">
            <img src='{img_url}' alt='{alt_text}' style='width:100%; border-radius:12px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);'>
            <figcaption style='color:#666; font-size:13px; text-align:center; margin-top:10px;'>Source: Unsplash ({keyword})</figcaption>
        </figure>
        """
    except: return ""

def inject_images(html_text, t1, t2, mode):
    fb_defaults = []
    if mode == "BIO":
        theme_instruction = "'laboratory', 'doctor', 'medicine', 'biology', 'DNA' ê°™ì´ ë°”ì´ì˜¤/ì˜ë£Œ ë¶„ì•¼ì™€ ê´€ë ¨ëœ ì§ê´€ì ì´ê³  ì‹œê°ì ì¸ ë²”ìš© ë‹¨ì–´"
        fb_defaults = ["biology laboratory", "medical research", "healthcare technology", "medicine", "dna structure", "biotech"]
    elif mode == "PATENT":
        theme_instruction = "'blueprint', 'patent', 'document', 'invention', 'innovation' ê°™ì´ íŠ¹í—ˆ/ë°œëª… ë¶„ì•¼ì™€ ê´€ë ¨ëœ ì§ê´€ì ì´ê³  ì‹œê°ì ì¸ ë²”ìš© ë‹¨ì–´"
        fb_defaults = ["blueprint architecture", "patent document", "technology invention", "business innovation", "future prototype", "design patent"]
    else: # TECH
        theme_instruction = "'technology', 'software', 'computer', 'digital', 'network' ê°™ì´ IT/í…Œí¬ ë¶„ì•¼ì™€ ê´€ë ¨ëœ ì§ê´€ì ì´ê³  ì‹œê°ì ì¸ ë²”ìš© ë‹¨ì–´"
        fb_defaults = ["digital technology", "software code", "future tech", "network data", "cyber security", "ai interface"]

    prompt = f"""
    Unsplash ì´ë¯¸ì§€ ê²€ìƒ‰ìš© ì˜ë¬¸ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì¤˜. ë³µì¡í•œ ê³ ìœ ëª…ì‚¬ë‚˜ íŠ¹ì • ë²ˆí˜¸ ë“±ì€ ëª¨ë‘ ë°°ì œí•˜ê³ , ë°˜ë“œì‹œ ë³¸ë¬¸ ë‚´ìš©ê³¼ ì—°ê´€ë˜ë©´ì„œ {theme_instruction} 3ê°œì”© ì´ 6ê°œ ì¶œë ¥í•´.
    ì•„ë˜ JSON í˜•ì‹ì— ë§ì¶°ì„œ 6ê°œì˜ í‚¤ì›Œë“œë¥¼ ì‘ì„±í•´ì¤˜. ê¸°í˜¸ ì—†ì´ ì˜ë¬¸ë§Œ ì‘ì„±.

    [ì£¼ì œ 1] {t1['title']}
    [ì£¼ì œ 2] {t2['title']}

    ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ JSON ì½”ë“œë§Œ ì¶œë ¥):
    {{
        "k1_1": "ì£¼ì œ1 ì²«ë²ˆì§¸ í‚¤ì›Œë“œ",
        "k1_2": "ì£¼ì œ1 ë‘ë²ˆì§¸ í‚¤ì›Œë“œ",
        "k1_3": "ì£¼ì œ1 ì„¸ë²ˆì§¸ í‚¤ì›Œë“œ",
        "k2_1": "ì£¼ì œ2 ì²«ë²ˆì§¸ í‚¤ì›Œë“œ",
        "k2_2": "ì£¼ì œ2 ë‘ë²ˆì§¸ í‚¤ì›Œë“œ",
        "k2_3": "ì£¼ì œ2 ì„¸ë²ˆì§¸ í‚¤ì›Œë“œ"
    }}
    """
    
    try:
        response_text = model.generate_content(prompt).text.strip()
        time.sleep(5) # API í˜¸ì¶œ ì œí•œ ë°©ì§€
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° ë° JSON íŒŒì‹±
        json_str = re.sub(r"```[a-zA-Z]*\n?|```", "", response_text).strip()
        keywords = json.loads(json_str)
        
        k1_1 = re.sub(r'[^a-zA-Z0-9\s]', '', keywords.get("k1_1", fb_defaults[0]))
        k1_2 = re.sub(r'[^a-zA-Z0-9\s]', '', keywords.get("k1_2", fb_defaults[1]))
        k1_3 = re.sub(r'[^a-zA-Z0-9\s]', '', keywords.get("k1_3", fb_defaults[2]))
        k2_1 = re.sub(r'[^a-zA-Z0-9\s]', '', keywords.get("k2_1", fb_defaults[3]))
        k2_2 = re.sub(r'[^a-zA-Z0-9\s]', '', keywords.get("k2_2", fb_defaults[4]))
        k2_3 = re.sub(r'[^a-zA-Z0-9\s]', '', keywords.get("k2_3", fb_defaults[5]))

    except Exception as e: 
        print(f"Keyword JSON parsing failed: {e}")
        k1_1, k1_2, k1_3 = fb_defaults[0], fb_defaults[1], fb_defaults[2]
        k2_1, k2_2, k2_3 = fb_defaults[3], fb_defaults[4], fb_defaults[5]
    
    used_urls = set() # ì¤‘ë³µ ê²€ì‚¬ë¥¼ ìœ„í•œ Set ì´ˆê¸°í™”
    
    html_text = html_text.replace("[IMAGE_PLACEHOLDER_1]", get_image_tag(k1_1, used_urls, t1['title']))
    html_text = html_text.replace("[IMAGE_PLACEHOLDER_2]", get_image_tag(k1_2, used_urls, "Analysis 1")) 
    html_text = html_text.replace("[IMAGE_PLACEHOLDER_3]", get_image_tag(k1_3, used_urls, "Analysis 2")) 
    html_text = html_text.replace("[IMAGE_PLACEHOLDER_4]", get_image_tag(k2_1, used_urls, t2['title']))
    html_text = html_text.replace("[IMAGE_PLACEHOLDER_5]", get_image_tag(k2_2, used_urls, "Market Insight 1"))
    html_text = html_text.replace("[IMAGE_PLACEHOLDER_6]", get_image_tag(k2_3, used_urls, "Market Insight 2"))
    return html_text

def generate_toc_and_add_ids(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    
    toc_html = "<div class='spo-toc' style='background-color: #f8f9fa; padding: 25px; border-radius: 12px; margin-bottom: 40px; border: 1px solid #e9ecef;'>\n"
    toc_html += "<h2 style='margin-top: 0; color: #2c3e50; font-size: 1.4em; border-bottom: 2px solid #3498db; padding-bottom: 10px; display: inline-block;'>ğŸ“‘ ëª©ì°¨</h2>\n"
    toc_html += "<ul style='list-style-type: none; padding-left: 0; margin-bottom: 0; line-height: 1.8;'>\n"
    
    headings = soup.find_all(['h1', 'h2'])
    for idx, tag in enumerate(headings):
        anchor_id = f"section-{idx}"
        tag['id'] = anchor_id
        text = tag.get_text(strip=True)
        
        if tag.name == 'h1':
            toc_html += f"<li style='margin-top: 15px; font-weight: bold; font-size: 1.1em;'><a href='#{anchor_id}' style='color: #2980b9; text-decoration: none;'>{text}</a></li>\n"
        elif tag.name == 'h2':
            toc_html += f"<li style='margin-top: 5px; margin-left: 20px; font-size: 0.95em;'><a href='#{anchor_id}' style='color: #34495e; text-decoration: none;'>- {text}</a></li>\n"
            
    toc_html += "</ul>\n</div>\n"
    return toc_html + str(soup)

def apply_namuwiki_tooltips(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    
    glossary_dict = {}
    glossary_header = soup.find(lambda tag: tag.name == 'h2' and 'ìš©ì–´ ì •ë¦¬' in tag.text)
    
    if glossary_header:
        list_tag = glossary_header.find_next_sibling(['ul', 'ol', 'dl'])
        if list_tag:
            for item in list_tag.find_all('li'):
                text = item.get_text(strip=True)
                if ':' in text:
                    parts = text.split(':', 1)
                elif '-' in text:
                    parts = text.split('-', 1)
                else:
                    continue
                    
                if len(parts) == 2:
                    term = parts[0].strip()
                    desc = parts[1].strip()
                    glossary_dict[term] = desc

    tooltip_css = """
    <style>
    .spo-tooltip-container {
        position: relative;
        display: inline-block;
        border-bottom: 2px dashed #3498db;
        color: #2980b9;
        cursor: pointer;
        font-weight: bold;
        text-decoration: none !important;
    }
    .spo-tooltip-container .spo-tooltip-text {
        visibility: hidden;
        width: max-content;
        max-width: 320px;
        background-color: #2c3e50;
        color: #ffffff;
        text-align: left;
        border-radius: 8px;
        padding: 10px 14px;
        position: absolute;
        z-index: 9999;
        bottom: 130%;
        left: 50%;
        transform: translateX(-50%);
        opacity: 0;
        transition: opacity 0.3s ease, transform 0.3s ease;
        font-size: 14px;
        font-weight: normal;
        line-height: 1.6;
        box-shadow: 0 4px 15px rgba(0,0,0,0.2);
        word-break: keep-all;
        white-space: pre-wrap;
    }
    .spo-tooltip-container .spo-tooltip-text::after {
        content: "";
        position: absolute;
        top: 100%;
        left: 50%;
        margin-left: -6px;
        border-width: 6px;
        border-style: solid;
        border-color: #2c3e50 transparent transparent transparent;
    }
    .spo-tooltip-container:hover .spo-tooltip-text,
    .spo-tooltip-container:active .spo-tooltip-text {
        visibility: visible;
        opacity: 1;
        transform: translateX(-50%) translateY(-3px);
    }
    </style>
    """

    if glossary_dict:
        for u_tag in soup.find_all('u'):
            term_text = u_tag.get_text(strip=True)
            
            matched_desc = None
            for key, desc in glossary_dict.items():
                if term_text.lower() in key.lower() or key.lower() in term_text.lower():
                    matched_desc = desc
                    break
            
            if matched_desc:
                span_container = soup.new_tag("span", attrs={"class": "spo-tooltip-container"})
                span_container.string = term_text
                
                span_tooltip = soup.new_tag("span", attrs={"class": "spo-tooltip-text"})
                span_tooltip.string = matched_desc
                
                span_container.append(span_tooltip)
                u_tag.replace_with(span_container)
                
    return tooltip_css + str(soup)

def send_email(subject, final_content):
    escaped_html = html.escape(final_content)
    email_body = f"""
    <div style="font-family: sans-serif; max-width: 800px; margin: 0 auto;">
        <h2 style="color: #2c3e50;">ìŠ¤í¬(spo) í¸ì§‘ì¥ë‹˜, ìƒˆ í¬ìŠ¤íŒ…ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ‰</h2>
        <p style="color: #e74c3c; font-weight: bold;">[í‹°ìŠ¤í† ë¦¬ ì—…ë¡œë“œìš© HTML ì½”ë“œ]</p>
        <textarea style="width: 100%; height: 200px; font-family: monospace; font-size: 13px; background-color: #f8f9fa; padding: 15px; border: 1px solid #ced4da; border-radius: 5px;" readonly>{escaped_html}</textarea>
        <hr style="border: 0; height: 1px; background: #ddd; margin: 40px 0;">
        <h3 style="color: #2c3e50;">ğŸ‘€ í¬ìŠ¤íŒ… ë¯¸ë¦¬ë³´ê¸°</h3>
        <div style="border: 1px solid #eee; padding: 30px; border-radius: 10px; background-color: #fff;">
            {final_content}
        </div>
    </div>
    """

    msg = MIMEMultipart()
    msg['From'] = GMAIL_USER
    msg['To'] = GMAIL_USER 
    msg['Subject'] = subject
    msg.attach(MIMEText(email_body, 'html'))
    
    try:
        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:
            server.login(GMAIL_USER, GMAIL_APP_PASSWORD)
            server.send_message(msg)
        print(f"âœ… Email Sent: {subject}")
    except Exception as e:
        print(f"âŒ Email Fail: {e}")

# --- 6. í†µí•© ì²˜ë¦¬ í•¨ìˆ˜ (ìˆ˜ì •ë¨) ---
def process_and_send(mode, category_korean, history):
    print(f"\n>>> Processing: {category_korean} ({mode})")
    candidates = get_candidates(mode)
    selected = select_top_2(candidates, history, category_korean)
    
    if len(selected) < 2:
        print(f"Not enough news for {mode}")
        return []
        
    t1_kr = get_catchy_korean_title(selected[0]['title'])
    t2_kr = get_catchy_korean_title(selected[1]['title'])
    
    raw_html = write_blog_post(selected[0], selected[1], category_korean, t1_kr, t2_kr)
    
    html_with_images = inject_images(raw_html, selected[0], selected[1], mode)
    
    html_with_toc = generate_toc_and_add_ids(html_with_images)
    
    html_with_tooltips = apply_namuwiki_tooltips(html_with_toc)
    
    final_tistory_content = f"""
    <div class="spo-analysis-report" style="line-height: 1.8; color: #333; font-family: 'Noto Sans KR', sans-serif; word-break: keep-all; padding: 10px;">
        {html_with_tooltips}
    </div>
    """
    
    # ìˆ˜ì •ëœ ë¶€ë¶„: ì´ë©”ì¼ ì „ì†¡ ì‹œ ìƒˆë¡œ ë§Œë“  í†µí•© ì œëª© í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    subject = get_unified_subject(category_korean, t1_kr, t2_kr)
    send_email(subject, final_tistory_content)
    
    return selected

# --- ë©”ì¸ ì‹¤í–‰ ---
def main():
    history_file = 'history.json'
    history = load_history(history_file)
    
    kst_now = datetime.datetime.now() + datetime.timedelta(hours=9)
    weekday = kst_now.weekday()
    
    new_items_total = []

    if weekday == 0: # ì›”ìš”ì¼
        items = process_and_send("TECH", "í…Œí¬", history)
        new_items_total.extend(items)
    else: # í™”~ì¼ìš”ì¼
        items_bio = process_and_send("BIO", "ë°”ì´ì˜¤", history)
        new_items_total.extend(items_bio)
        items_patent = process_and_send("PATENT", "íŠ¹í—ˆ", history)
        new_items_total.extend(items_patent)
    
    if new_items_total:
        save_history(history_file, history, new_items_total)

if __name__ == "__main__":
    main()
