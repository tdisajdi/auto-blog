import os
import json
import datetime
import time
import requests
import feedparser
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import google.generativeai as genai 
import re
import html
from bs4 import BeautifulSoup

# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ---
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
UNSPLASH_ACCESS_KEY = os.environ.get("UNSPLASH_ACCESS_KEY")
GMAIL_USER = os.environ.get("GMAIL_USER")
GMAIL_APP_PASSWORD = os.environ.get("GMAIL_APP_PASSWORD")

# Gemini ì„¤ì •
genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-3-flash-preview')

# --- 0. íˆìŠ¤í† ë¦¬ ê´€ë¦¬ ---
def load_history(filepath):
    if not os.path.exists(filepath): return []
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except: return []

def save_history(filepath, history, new_items):
    cutoff = datetime.datetime.now() - datetime.timedelta(days=30)
    cleaned = []
    
    for item in history:
        try:
            d = datetime.datetime.strptime(item.get('date', '2000-01-01'), "%Y-%m-%d")
            if d >= cutoff: cleaned.append(item)
        except: continue
        
    today = datetime.datetime.now().strftime("%Y-%m-%d")
    for item in new_items:
        cleaned.append({"id": item['id'], "title": item['title'], "date": today})
        
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(cleaned, f, ensure_ascii=False, indent=4)

# --- 1. ë°ì´í„° ìˆ˜ì§‘ (ì›¹ ìŠ¤í¬ë˜í•‘ ì¶”ê°€) ---
def scrape_article_text(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        res = requests.get(url, headers=headers, timeout=5)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'html.parser')
        paragraphs = soup.find_all('p')
        text = " ".join([p.get_text() for p in paragraphs])
        return text[:3000] if len(text) > 100 else None 
    except Exception as e:
        print(f"Scraping failed for {url}: {e}")
        return None

def fetch_rss(url, category):
    items = []
    try:
        feed = feedparser.parse(url)
        cutoff = datetime.datetime.now() - datetime.timedelta(days=3)
        for entry in feed.entries:
            if 'published_parsed' in entry and entry.published_parsed:
                pub_date = datetime.datetime.fromtimestamp(time.mktime(entry.published_parsed))
                if pub_date < cutoff: continue
            
            print(f"Scraping: {entry.title}")
            raw_text = scrape_article_text(entry.link)
            if not raw_text:
                raw_text = (entry.summary if 'summary' in entry else entry.title)[:2000]
            
            items.append({
                "id": entry.link,
                "title": entry.title,
                "type": category,
                "raw": raw_text
            })
    except Exception as e:
        print(f"RSS Error ({url}): {e}")
    return items

def get_candidates(mode):
    items = []
    if mode == "TECH":
        urls = ["https://www.theverge.com/rss/index.xml", "https://techcrunch.com/feed/"]
    elif mode == "BIO":
        urls = ["https://news.google.com/rss/search?q=Biotech+OR+%22FDA+approval%22+OR+%22Clinical+Trial%22&hl=en-US&gl=US&ceid=US:en"]
    elif mode == "PATENT":
        urls = ["https://news.google.com/rss/search?q=Patent+OR+%22Technology+Innovation%22+OR+%22Future+Tech%22&hl=en-US&gl=US&ceid=US:en"]
    
    for u in urls: items.extend(fetch_rss(u, mode))
    return items

# --- 2. ì£¼ì œ ì„ ì • ---
def select_top_2(candidates, history, category_name):
    history_ids = [h['id'] for h in history]
    filtered = [c for c in candidates if c['id'] not in history_ids]
    
    if len(filtered) < 2: return filtered[:2]
    
    cand_txt = "\n".join([f"{i}. {c['title']}" for i, c in enumerate(filtered[:15])])
    
    prompt = f"""
    ì—­í• : ì „ë¬¸ íˆ¬ì/ê¸°ìˆ  ë¸”ë¡œê·¸ í¸ì§‘ì¥ 'ìŠ¤í¬(spo)'.
    ëª©í‘œ: {category_name} ë¶„ì•¼ì—ì„œ ì‹¬ì¸µ ë¶„ì„(Deep-Dive)ì´ ê°€ëŠ¥í•˜ê³  íˆ¬ììë“¤ì˜ ê´€ì‹¬ì´ ì§‘ì¤‘ë  ë‰´ìŠ¤ 2ê°œ ì„ ì •.
    
    [í›„ë³´êµ°]
    {cand_txt}
    
    ì¡°ê±´:
    1. ê¸°ìˆ ì  ì›ë¦¬ë‚˜ ì‹œì¥ íŒŒê¸‰ë ¥ì„ ë¶„ì„í•  ê±°ë¦¬ê°€ ìˆëŠ” ì£¼ì œ ìš°ì„ .
    2. ì˜¤ì§ ìˆ«ì 2ê°œë§Œ ë°˜í™˜ (ì˜ˆ: 1, 4).
    """
    try:
        res = model.generate_content(prompt)
        nums = [int(s) for s in re.findall(r'\b\d+\b', res.text)]
        if len(nums) >= 2:
            return [filtered[nums[0]], filtered[nums[1]]]
    except: pass
    return filtered[:2]

# --- 3. ë§¤ë ¥ì ì¸ í•œêµ­ì–´ ì œëª© ìƒì„± í•¨ìˆ˜ ---
def get_catchy_korean_title(english_title):
    prompt = f"""
    ë‹¤ìŒ ì˜ë¬¸ ë‰´ìŠ¤ ì œëª©ì„ ë²ˆì—­í•˜ë˜, ì‚¬ëŒë“¤ì˜ í˜¸ê¸°ì‹¬ì„ ê·¹ëŒ€í™”í•˜ê³  í´ë¦­ë¥ (CTR)ì„ ë†’ì¼ ìˆ˜ ìˆëŠ” ë§¤ë ¥ì ì´ê³  íŠ¸ë Œë””í•œ í•œêµ­ì–´ ë¸”ë¡œê·¸ ì œëª©ìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜.
    
    [ì¡°ê±´]
    1. ë¬´ì¡°ê±´ 100% í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•  ê²ƒ (ë¶ˆê°€í”¼í•œ ê³ ìœ ëª…ì‚¬ ì œì™¸).
    2. ì›ë¬¸ì˜ í•µì‹¬ì„ ì‚´ë¦¬ë˜, í¥ë¯¸ë¥¼ ìœ ë°œí•˜ëŠ” í›„í‚¹(hooking) ìš”ì†Œë¥¼ ì¶”ê°€í•  ê²ƒ (ì˜ˆ: "ê²°êµ­ í•´ëƒˆë‹¤", "ì‹œì¥ íŒë„ ë°”ê¿€ê¹Œ?", "ì¶©ê²©ì ì¸ ê²°ê³¼" ë“±).
    3. íŠ¹ìˆ˜ê¸°í˜¸(!, ?, [])ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ì‹œì„ ì„ ëŒ ê²ƒ.
    4. ë‹¤ë¥¸ ë¶€ê°€ ì„¤ëª… ì—†ì´ ì˜¤ì§ ìƒì„±ëœ 'ì œëª© 1ê°œ'ë§Œ ì¶œë ¥í•  ê²ƒ.
    
    ì˜ë¬¸ ì œëª©: {english_title}
    """
    try:
        return model.generate_content(prompt).text.strip()
    except:
        return english_title

# --- 4. ê¸€ ì‘ì„± ---
def write_blog_post(topic1, topic2, category_name, t1_kr, t2_kr):
    print(f"Writing {category_name} Post with Gemini...")
    
    # [ìˆ˜ì •ë¨] 10ë…„ ì°¨ í˜„ì—… ë² í…Œë‘ ì „ë¬¸ê°€ í˜ë¥´ì†Œë‚˜ ë° ì–´ì¡° ì§€ì¹¨ ê°•í™”
    tone_rule = """
    [êµ¬ê¸€ ì• ë“œì„¼ìŠ¤ ìŠ¹ì¸ ë° ê³ í’ˆì§ˆ ì½˜í…ì¸ ë¥¼ ìœ„í•œ í•„ìˆ˜ ë¬¸ì²´ ë° ì–´ì¡° ì§€ì¹¨ (10ë…„ì°¨ í˜„ì—… ì „ë¬¸ê°€ í˜ë¥´ì†Œë‚˜)]
    1. í˜ë¥´ì†Œë‚˜: ê´€ë ¨ ì—…ê³„ì—ì„œ 10ë…„ ì´ìƒ êµ¬ë¥´ë©°(?) ì‚°ì „ìˆ˜ì „ ë‹¤ ê²ªì€ ë² í…Œë‘ ì‹¤ë¬´ ì „ë¬¸ê°€. ì§€ë£¨í•œ êµê³¼ì„œì  ì„¤ëª…ì´ ì•„ë‹Œ, í˜„ì—…ì˜ 'ì§„ì§œ ëŒì•„ê°€ëŠ” ì´ì•¼ê¸°'ë¥¼ íŠ¸ë Œë””í•˜ê³  ê°ê°ì ìœ¼ë¡œ í’€ì–´ëƒ…ë‹ˆë‹¤.
    2. ë¬¸ì²´: êµ°ë”ë”ê¸° ì—†ì´ ê¹”ë”í•˜ê³  ê°€ë…ì„± ë†’ì€ ë¬¸ì¥ì„ êµ¬ì‚¬í•©ë‹ˆë‹¤. "~ìŠµë‹ˆë‹¤", "~í•˜ì£ " ë“± ì‹ ë¢°ê° ìˆëŠ” ê²½ì–´ì²´ë¥¼ ì‚¬ìš©í•˜ë˜, ì§€ì¸ì—ê²Œ ê³ ê¸‰ ì‹¤ë¬´ ì •ë³´ë¥¼ ìŠ¬ì© ê³µìœ í•´ì£¼ë“¯ ìì—°ìŠ¤ëŸ½ê³  ì „ë¬¸ê°€ íŠ¹ìœ ì˜ ì—¬ìœ ê°€ ë¬»ì–´ë‚˜ëŠ” ì–´íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    3. ì£¼ê´€ì ì´ê³  ì˜ˆë¦¬í•œ ë¶„ì„: ë‹¨ìˆœ ì‚¬ì‹¤ ì „ë‹¬ì„ ë„˜ì–´, "ì†”ì§íˆ ì´ë²ˆ ì´ìŠˆë¡œ ë³¼ ë•Œ Aì‚¬ë³´ë‹¤ëŠ” Bì‚¬ê°€ ì‹œì¥ ì„ ì ì— í›¨ì”¬ ìœ ë¦¬í•œ ê³ ì§€ë¥¼ ì°¨ì§€í•  ê²ë‹ˆë‹¤. í˜„ì—…ì—ì„œ ê·¸ë ‡ê²Œ ë³´ëŠ” ì´ìœ ëŠ”..."ê³¼ ê°™ì´ 10ë…„ì°¨ íŠ¹ìœ ì˜ ëšœë ·í•œ ì£¼ê´€ê³¼ ì˜ˆë¦¬í•œ ë¹„êµ ë¶„ì„ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.
    4. AI ë§íˆ¬ 200% ê¸ˆì§€: 'ê²°ë¡ ì ìœ¼ë¡œ', 'ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤', 'ì´ ê¸°ì‚¬ë¥¼ í†µí•´', 'ì•ˆë…•í•˜ì„¸ìš”', 'ìš”ì•½í•˜ìë©´', 'í¥ë¯¸ì§„ì§„í•œ' ë“± AI íŠ¹ìœ ì˜ ìƒíˆ¬ì ì´ê³  ì˜í˜¼ ì—†ëŠ” í‘œí˜„ì€ ì ˆëŒ€ ê¸ˆì§€í•©ë‹ˆë‹¤. ì§„ì§œ ì‚¬ëŒì´ ì“´ ê²ƒì²˜ëŸ¼ ë¬¸ë‹¨ ê°„ ì—°ê²°ì„ ë§¤ë„ëŸ½ê²Œ í•˜ì„¸ìš”.
    """

    structure_instruction = """
    ê° ì£¼ì œë³„ë¡œ ë°˜ë“œì‹œ ì•„ë˜ 7ê°€ì§€ H2 íƒœê·¸ ì„¹ì…˜ì„ í¬í•¨í•´ì•¼ í•¨:
    1. <h2>1. ë°°ê²½ ë° ê°œìš” (The Context)</h2> : í˜„ ìƒí™©ì„ ë»”í•˜ì§€ ì•Šê²Œ 3ì¤„ ìš”ì•½ ë¦¬ìŠ¤íŠ¸(<ul>)ë¡œ ì œì‹œ.
    2. <h2>2. ê¸°ì¡´ ê¸°ìˆ /ì•½ë¬¼ê³¼ì˜ ì°¨ë³„ì  (Comparative Analysis)</h2> : ê³¼ê±° ìœ ì‚¬í–ˆë˜ ì‚¬ë¡€ì™€ ë¹„êµí•˜ì—¬ ì´ë²ˆ ì£¼ì œì˜ ì§„ì§œ í˜ì‹  í¬ì¸íŠ¸ê°€ ë¬´ì—‡ì¸ì§€ ì—ë””í„°ì˜ ì‹œê°ìœ¼ë¡œ ë¶„ì„.
    3. <h2>3. ê¸°ìˆ ì  ë©”ì»¤ë‹ˆì¦˜ (Technical Deep-Dive)</h2> : <table>ì„ 1ê°œ ì´ìƒ ë°˜ë“œì‹œ í¬í•¨. ì „ë¬¸ì ì´ì§€ë§Œ ë…ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì ì ˆí•œ ë¹„ìœ ë¥¼ ì„ì–´ ì„¤ëª….
    4. <h2>4. ì‹œì¥ íŒë„ ë° ê²½ìŸì‚¬ ë¶„ì„ (Market Dynamics)</h2> : [ë§¤ìš° ì¤‘ìš”] ê°ê´€ì ì¸ ë°ì´í„°ì™€ í•¨ê»˜, "A ê¸°ì—…ë³´ë‹¤ B ê¸°ì—…ì´ ì´ êµ­ë©´ì—ì„œ ì™œ ë” ìœ ë¦¬í•œì§€", í˜¹ì€ "ê¸°ì¡´ ê°•ì C ê¸°ì—…ì—ê²Œ ì–´ë–¤ ì¹˜ëª…ì ì¸ ìœ„í˜‘ì´ ë ì§€" ë“± êµ¬ì²´ì ì´ê³  ì£¼ê´€ì ì¸ ê¸°ì—…/ê¸°ìˆ  ê°„ ìš°ìœ„ ë¶„ì„ì„ ë°˜ë“œì‹œ ì‘ì„±.
    5. <h2>5. ë¦¬ìŠ¤í¬ ë° í•œê³„ì  (Risk Factors)</h2> : í‘œë©´ì ì¸ ë¦¬ìŠ¤í¬ê°€ ì•„ë‹Œ, ì‹¤ë¬´ì/íˆ¬ìì ê´€ì ì—ì„œì˜ ì§„ì§œ ê±¸ë¦¼ëŒ(ê·œì œ, ê²½ìŸ ì‹¬í™”, ê¸°ìˆ ì  ì¥ë²½ ë“±)ì„ ì˜ˆë¦¬í•˜ê²Œ ì§€ì .
    6. <h2>6. ê¸ì •ì  ì „ë§ ë° ê¸°ëŒ€ íš¨ê³¼ (Future Hope & Impact)</h2> : ì´ ë³€í™”ê°€ ê°€ì ¸ì˜¬ ë¯¸ë˜ ì‚°ì—…ì˜ ëª¨ìŠµì„ ìƒìƒí•˜ê²Œ ê·¸ë ¤ì£¼ë“¯ ì„œìˆ .
    7. <h2>7. ìŠ¤í¬(spo)ì˜ ì¸ì‚¬ì´íŠ¸ (Actionable Insights)</h2> : ë‹¨ìˆœ ìš”ì•½ ê¸ˆì§€. "ê·¸ë˜ì„œ ì§€ê¸ˆ ìš°ë¦¬ëŠ” ë¬´ì—‡ì„ ì£¼ëª©í•´ì•¼ í•˜ëŠ”ê°€?"ì— ëŒ€í•œ ì—ë””í„° ìŠ¤í¬ì˜ ë§¤ìš° ì£¼ê´€ì ì´ê³  ì‚¬ëŒ ëƒ„ìƒˆ ë‚˜ëŠ” ì†”ì§í•œ ì´í‰ê³¼ íˆ¬ì/ì‚°ì—…ì  ì¡°ì–¸.
    """
    glossary_rule = "ì–´ë ¤ìš´ 'ì „ë¬¸ ìš©ì–´'ëŠ” ë°˜ë“œì‹œ <u> íƒœê·¸ë¡œ ê°ì‹¸ì£¼ì„¸ìš”."
    bold_rule = "ê°€ë…ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ë¬¸ë‹¨ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ 'í•µì‹¬ ë¬¸ì¥'ê³¼ 'ì£¼ìš” í‚¤ì›Œë“œ(ë‹¨ì–´)'ëŠ” ë°˜ë“œì‹œ <b> íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ êµµê²Œ ê°•ì¡°í•´ì£¼ì„¸ìš”."

    outline = model.generate_content(f"ì£¼ì œ1: {topic1['title']}\nì£¼ì œ2: {topic2['title']}\nìœ„ ë‘ ì£¼ì œë¡œ '{category_name} ì‹¬ì¸µ ë¶„ì„' ê°œìš” ì‘ì„±.").text
    
    # [ìˆ˜ì •ë¨] í˜ë¥´ì†Œë‚˜ë¥¼ ë” ëª…í™•í•˜ê²Œ ë¶€ì—¬
    p1_prompt = f"""
    ì—­í• : {category_name} ì—…ê³„ 10ë…„ì°¨ í˜„ì—… ì „ë¬¸ê°€ì´ì, íŠ¸ë Œë””í•˜ê³  ê¹”ë”í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ëŠ” ì‹¤ë¬´ ë¶„ì„ê°€ 'ìŠ¤í¬(spo)'.
    ê°œìš”: {outline}
    ì£¼ì œ 1: {topic1['title']} / ì›ë¬¸ ë‚´ìš©: {topic1['raw']}
    {tone_rule}
    {glossary_rule}\n{bold_rule}
    [ì‘ì„± ì§€ì¹¨] HTML íƒœê·¸ë§Œ ì¶œë ¥.
    <h1>[{category_name} ì‹¬ì¸µë¶„ì„] {t1_kr}</h1>
    [IMAGE_PLACEHOLDER_1]
    {structure_instruction}
    [IMAGE_PLACEHOLDER_2]
    ì£¼ì œ 1ì˜ ë‚´ìš©ë§Œ ì‘ì„±.
    """
    part1 = re.sub(r"```[a-zA-Z]*\n?|```", "", model.generate_content(p1_prompt).text).strip()
    
    p2_prompt = f"""
    ì•ë¶€ë¶„: {part1}
    ì£¼ì œ 2: {topic2['title']} / ì›ë¬¸ ë‚´ìš©: {topic2['raw']}
    {tone_rule}
    {glossary_rule}\n{bold_rule}
    [ì‘ì„± ì§€ì¹¨] ì• ë‚´ìš©ê³¼ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ë„ë¡ ì‘ì„±. HTML íƒœê·¸ë§Œ ì¶œë ¥.
    <br><hr style="border: 0; height: 1px; background: #ddd; margin: 40px 0;"><br>
    <h1>[{category_name} ì‹¬ì¸µë¶„ì„] {t2_kr}</h1>
    [IMAGE_PLACEHOLDER_3]
    {structure_instruction}
    [IMAGE_PLACEHOLDER_4]
    <br><hr style="border: 0; height: 2px; background: #2c3e50; margin: 50px 0;"><br>
    <h2>ğŸ¯ í†µí•© ì¸ì‚¬ì´íŠ¸: ë‘ ë‰´ìŠ¤ê°€ ê·¸ë¦¬ëŠ” ë¯¸ë˜ (The Bridge)</h2>
    <h2>ğŸ“– ì˜¤ëŠ˜ì˜ ìš©ì–´ ì •ë¦¬ (Glossary)</h2>
    <h2>ğŸ” SEO ë° íƒœê·¸ ì •ë³´ (ì—…ë¡œë“œìš©)</h2>
    <hr style="border: 0; height: 1px; background: #eee; margin: 40px 0;">
    <p style="color:grey; font-size: 0.9em; text-align: center;">* ë³¸ ì½˜í…ì¸ ëŠ” ì •ë³´ ì œê³µì„ ëª©ì ìœ¼ë¡œ í•˜ë©°, íˆ¬ìì˜ ì±…ì„ì€ ë³¸ì¸ì—ê²Œ ìˆìŠµë‹ˆë‹¤. <br> Editor: ìŠ¤í¬(spo)</p>
    """
    part2 = re.sub(r"```[a-zA-Z]*\n?|```", "", model.generate_content(p2_prompt).text).strip()
    
    return part1 + "\n" + part2

# --- 5. ì´ë¯¸ì§€, ëª©ì°¨ ìƒì„± ë° ì´ë©”ì¼ ì „ì†¡ ---
def get_image_tag(keyword, alt_text=""):
    search_query = f"{keyword}"
    url = f"https://api.unsplash.com/search/photos?query={search_query}&per_page=1&orientation=landscape&client_id={UNSPLASH_ACCESS_KEY}"
    try:
        data = requests.get(url, timeout=5).json()
        if not data['results']: 
            return ""
        img_url = data['results'][0]['urls']['regular']
        return f"""
        <figure style="margin: 30px 0;">
            <img src='{img_url}' alt='{alt_text}' style='width:100%; border-radius:12px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);'>
            <figcaption style='color:#666; font-size:13px; text-align:center; margin-top:10px;'>Source: Unsplash ({keyword})</figcaption>
        </figure>
        """
    except: return ""

def inject_images(html_text, t1, t2, mode):
    if mode == "BIO":
        theme_instruction = "'laboratory', 'doctor', 'medicine', 'biology', 'DNA' ê°™ì´ ë°”ì´ì˜¤/ì˜ë£Œ ë¶„ì•¼ì™€ ê´€ë ¨ëœ ì§ê´€ì ì´ê³  ì‹œê°ì ì¸ ë²”ìš© ë‹¨ì–´"
        fb1_m, fb1_s = "biology laboratory", "medical research"
        fb2_m, fb2_s = "healthcare technology", "medicine"
    elif mode == "PATENT":
        theme_instruction = "'blueprint', 'patent', 'document', 'invention', 'innovation' ê°™ì´ íŠ¹í—ˆ/ë°œëª… ë¶„ì•¼ì™€ ê´€ë ¨ëœ ì§ê´€ì ì´ê³  ì‹œê°ì ì¸ ë²”ìš© ë‹¨ì–´"
        fb1_m, fb1_s = "blueprint architecture", "patent document"
        fb2_m, fb2_s = "technology invention", "business innovation"
    else: # TECH
        theme_instruction = "'technology', 'software', 'computer', 'digital', 'network' ê°™ì´ IT/í…Œí¬ ë¶„ì•¼ì™€ ê´€ë ¨ëœ ì§ê´€ì ì´ê³  ì‹œê°ì ì¸ ë²”ìš© ë‹¨ì–´"
        fb1_m, fb1_s = "digital technology", "software code"
        fb2_m, fb2_s = "future tech", "network data"

    prompt = f"Unsplash ì´ë¯¸ì§€ ê²€ìƒ‰ìš© ì˜ë¬¸ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì¤˜. ë³µì¡í•œ ê³ ìœ ëª…ì‚¬ë‚˜ íŠ¹ì • ë²ˆí˜¸ ë“±ì€ ëª¨ë‘ ë°°ì œí•˜ê³ , ë°˜ë“œì‹œ ë³¸ë¬¸ ë‚´ìš©ê³¼ ì—°ê´€ë˜ë©´ì„œ {theme_instruction} 1~2ê°œë§Œ ì¶œë ¥í•´. ê¸°í˜¸ ì—†ì´ ì˜ë¬¸ë§Œ ì¶œë ¥:\n{{}}"
    
    try:
        k1_main = re.sub(r'[^a-zA-Z0-9\s]', '', model.generate_content(prompt.format(t1['title'])).text.strip())
        k1_sub = re.sub(r'[^a-zA-Z0-9\s]', '', model.generate_content(prompt.format(t1['title'] + " (Focus on abstract business or data concept)")).text.strip())
        k2_main = re.sub(r'[^a-zA-Z0-9\s]', '', model.generate_content(prompt.format(t2['title'])).text.strip())
        k2_sub = re.sub(r'[^a-zA-Z0-9\s]', '', model.generate_content(prompt.format(t2['title'] + " (Focus on abstract business or data concept)")).text.strip())
        
        # í‚¤ì›Œë“œ ìƒì„±ì´ ì œëŒ€ë¡œ ì•ˆ ë˜ì—ˆì„ ë•Œë¥¼ ëŒ€ë¹„í•œ ì•ˆì „ ì¥ì¹˜
        if not k1_main: k1_main = fb1_m
        if not k1_sub: k1_sub = fb1_s
        if not k2_main: k2_main = fb2_m
        if not k2_sub: k2_sub = fb2_s
    except: 
        k1_main, k1_sub = fb1_m, fb1_s
        k2_main, k2_sub = fb2_m, fb2_s
    
    html_text = html_text.replace("[IMAGE_PLACEHOLDER_1]", get_image_tag(k1_main, t1['title']))
    html_text = html_text.replace("[IMAGE_PLACEHOLDER_2]", get_image_tag(k1_sub, "Analysis")) 
    html_text = html_text.replace("[IMAGE_PLACEHOLDER_3]", get_image_tag(k2_main, t2['title']))
    html_text = html_text.replace("[IMAGE_PLACEHOLDER_4]", get_image_tag(k2_sub, "Market Insight"))
    return html_text

def generate_toc_and_add_ids(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    
    toc_html = "<div class='spo-toc' style='background-color: #f8f9fa; padding: 25px; border-radius: 12px; margin-bottom: 40px; border: 1px solid #e9ecef;'>\n"
    toc_html += "<h2 style='margin-top: 0; color: #2c3e50; font-size: 1.4em; border-bottom: 2px solid #3498db; padding-bottom: 10px; display: inline-block;'>ğŸ“‘ ëª©ì°¨</h2>\n"
    toc_html += "<ul style='list-style-type: none; padding-left: 0; margin-bottom: 0; line-height: 1.8;'>\n"
    
    headings = soup.find_all(['h1', 'h2'])
    for idx, tag in enumerate(headings):
        anchor_id = f"section-{idx}"
        tag['id'] = anchor_id
        text = tag.get_text(strip=True)
        
        if tag.name == 'h1':
            toc_html += f"<li style='margin-top: 15px; font-weight: bold; font-size: 1.1em;'><a href='#{anchor_id}' style='color: #2980b9; text-decoration: none;'>{text}</a></li>\n"
        elif tag.name == 'h2':
            toc_html += f"<li style='margin-top: 5px; margin-left: 20px; font-size: 0.95em;'><a href='#{anchor_id}' style='color: #34495e; text-decoration: none;'>- {text}</a></li>\n"
            
    toc_html += "</ul>\n</div>\n"
    return toc_html + str(soup)

def apply_namuwiki_tooltips(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    
    glossary_dict = {}
    glossary_header = soup.find(lambda tag: tag.name == 'h2' and 'ìš©ì–´ ì •ë¦¬' in tag.text)
    
    if glossary_header:
        list_tag = glossary_header.find_next_sibling(['ul', 'ol', 'dl'])
        if list_tag:
            for item in list_tag.find_all('li'):
                text = item.get_text(strip=True)
                if ':' in text:
                    parts = text.split(':', 1)
                elif '-' in text:
                    parts = text.split('-', 1)
                else:
                    continue
                    
                if len(parts) == 2:
                    term = parts[0].strip()
                    desc = parts[1].strip()
                    glossary_dict[term] = desc

    tooltip_css = """
    <style>
    .spo-tooltip-container {
        position: relative;
        display: inline-block;
        border-bottom: 2px dashed #3498db;
        color: #2980b9;
        cursor: pointer;
        font-weight: bold;
        text-decoration: none !important;
    }
    .spo-tooltip-container .spo-tooltip-text {
        visibility: hidden;
        width: max-content;
        max-width: 320px;
        background-color: #2c3e50;
        color: #ffffff;
        text-align: left;
        border-radius: 8px;
        padding: 10px 14px;
        position: absolute;
        z-index: 9999;
        bottom: 130%;
        left: 50%;
        transform: translateX(-50%);
        opacity: 0;
        transition: opacity 0.3s ease, transform 0.3s ease;
        font-size: 14px;
        font-weight: normal;
        line-height: 1.6;
        box-shadow: 0 4px 15px rgba(0,0,0,0.2);
        word-break: keep-all;
        white-space: pre-wrap;
    }
    .spo-tooltip-container .spo-tooltip-text::after {
        content: "";
        position: absolute;
        top: 100%;
        left: 50%;
        margin-left: -6px;
        border-width: 6px;
        border-style: solid;
        border-color: #2c3e50 transparent transparent transparent;
    }
    .spo-tooltip-container:hover .spo-tooltip-text,
    .spo-tooltip-container:active .spo-tooltip-text {
        visibility: visible;
        opacity: 1;
        transform: translateX(-50%) translateY(-3px);
    }
    </style>
    """

    if glossary_dict:
        for u_tag in soup.find_all('u'):
            term_text = u_tag.get_text(strip=True)
            
            matched_desc = None
            for key, desc in glossary_dict.items():
                if term_text.lower() in key.lower() or key.lower() in term_text.lower():
                    matched_desc = desc
                    break
            
            if matched_desc:
                span_container = soup.new_tag("span", attrs={"class": "spo-tooltip-container"})
                span_container.string = term_text
                
                span_tooltip = soup.new_tag("span", attrs={"class": "spo-tooltip-text"})
                span_tooltip.string = matched_desc
                
                span_container.append(span_tooltip)
                u_tag.replace_with(span_container)
                
    return tooltip_css + str(soup)

def send_email(subject, final_content):
    escaped_html = html.escape(final_content)
    email_body = f"""
    <div style="font-family: sans-serif; max-width: 800px; margin: 0 auto;">
        <h2 style="color: #2c3e50;">ìŠ¤í¬(spo) í¸ì§‘ì¥ë‹˜, ìƒˆ í¬ìŠ¤íŒ…ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ‰</h2>
        <p style="color: #e74c3c; font-weight: bold;">[í‹°ìŠ¤í† ë¦¬ ì—…ë¡œë“œìš© HTML ì½”ë“œ]</p>
        <textarea style="width: 100%; height: 200px; font-family: monospace; font-size: 13px; background-color: #f8f9fa; padding: 15px; border: 1px solid #ced4da; border-radius: 5px;" readonly>{escaped_html}</textarea>
        <hr style="border: 0; height: 1px; background: #ddd; margin: 40px 0;">
        <h3 style="color: #2c3e50;">ğŸ‘€ í¬ìŠ¤íŒ… ë¯¸ë¦¬ë³´ê¸°</h3>
        <div style="border: 1px solid #eee; padding: 30px; border-radius: 10px; background-color: #fff;">
            {final_content}
        </div>
    </div>
    """

    msg = MIMEMultipart()
    msg['From'] = GMAIL_USER
    msg['To'] = GMAIL_USER 
    msg['Subject'] = subject
    msg.attach(MIMEText(email_body, 'html'))
    
    try:
        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:
            server.login(GMAIL_USER, GMAIL_APP_PASSWORD)
            server.send_message(msg)
        print(f"âœ… Email Sent: {subject}")
    except Exception as e:
        print(f"âŒ Email Fail: {e}")

# --- 6. í†µí•© ì²˜ë¦¬ í•¨ìˆ˜ ---
def process_and_send(mode, category_korean, history):
    print(f"\n>>> Processing: {category_korean} ({mode})")
    candidates = get_candidates(mode)
    selected = select_top_2(candidates, history, category_korean)
    
    if len(selected) < 2:
        print(f"Not enough news for {mode}")
        return []
        
    t1_kr = get_catchy_korean_title(selected[0]['title'])
    t2_kr = get_catchy_korean_title(selected[1]['title'])
    
    raw_html = write_blog_post(selected[0], selected[1], category_korean, t1_kr, t2_kr)
    
    html_with_images = inject_images(raw_html, selected[0], selected[1], mode)
    
    html_with_toc = generate_toc_and_add_ids(html_with_images)
    
    html_with_tooltips = apply_namuwiki_tooltips(html_with_toc)
    
    final_tistory_content = f"""
    <div class="spo-analysis-report" style="line-height: 1.8; color: #333; font-family: 'Noto Sans KR', sans-serif; word-break: keep-all; padding: 10px;">
        {html_with_tooltips}
    </div>
    """
    
    subject = f"[{category_korean} ë¶„ì„] {t1_kr} & {t2_kr}"
    send_email(subject, final_tistory_content)
    
    return selected

# --- ë©”ì¸ ì‹¤í–‰ ---
def main():
    history_file = 'history.json'
    history = load_history(history_file)
    
    kst_now = datetime.datetime.now() + datetime.timedelta(hours=9)
    weekday = kst_now.weekday()
    
    new_items_total = []

    if weekday == 0: # ì›”ìš”ì¼
        items = process_and_send("TECH", "í…Œí¬", history)
        new_items_total.extend(items)
    else: # í™”~ì¼ìš”ì¼
        items_bio = process_and_send("BIO", "ë°”ì´ì˜¤", history)
        new_items_total.extend(items_bio)
        items_patent = process_and_send("PATENT", "íŠ¹í—ˆ", history)
        new_items_total.extend(items_patent)
    
    if new_items_total:
        save_history(history_file, history, new_items_total)

if __name__ == "__main__":
    main()
